{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35a9d276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import recall_score\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cc80d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 클래스 정의\n",
    "class FoodDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx]['image_path']\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.dataframe.iloc[idx]['label']\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7c60ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall 계산 함수\n",
    "def calculate_recall(preds, labels):\n",
    "    preds = torch.sigmoid(preds).cpu().numpy()\n",
    "    preds = np.round(preds)\n",
    "    labels = labels.cpu().numpy()\n",
    "    return recall_score(labels, preds, zero_division=0, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32ebfa5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 10, Training Loss: 0.677469789981842\n",
      "Epoch 1, Batch 20, Training Loss: 0.6543619632720947\n",
      "Epoch 1, Batch 30, Training Loss: 0.6065874099731445\n",
      "Epoch 1, Batch 40, Training Loss: 0.6472185850143433\n",
      "Epoch 1, Batch 50, Training Loss: 0.6304143071174622\n",
      "Epoch 1, Batch 60, Training Loss: 0.632469117641449\n",
      "Epoch 1, Batch 70, Training Loss: 0.6146209239959717\n",
      "Epoch 1, Batch 80, Training Loss: 0.5429264903068542\n",
      "Epoch 1, Batch 90, Training Loss: 0.6830814480781555\n",
      "Epoch 1, Batch 100, Training Loss: 0.6189112067222595\n",
      "Epoch 1, Batch 110, Training Loss: 0.5077006816864014\n",
      "Epoch 1, Average Recall: 0.36679638610763454\n",
      "Epoch 2, Batch 10, Training Loss: 0.504433274269104\n",
      "Epoch 2, Batch 20, Training Loss: 0.47505462169647217\n",
      "Epoch 2, Batch 30, Training Loss: 0.4459352493286133\n",
      "Epoch 2, Batch 40, Training Loss: 0.40818148851394653\n",
      "Epoch 2, Batch 50, Training Loss: 0.5736027956008911\n",
      "Epoch 2, Batch 60, Training Loss: 0.44962480664253235\n",
      "Epoch 2, Batch 70, Training Loss: 0.4986337125301361\n",
      "Epoch 2, Batch 80, Training Loss: 0.478118360042572\n",
      "Epoch 2, Batch 90, Training Loss: 0.42440831661224365\n",
      "Epoch 2, Batch 100, Training Loss: 0.4203110635280609\n",
      "Epoch 2, Batch 110, Training Loss: 0.4492335021495819\n",
      "Epoch 2, Average Recall: 0.3817191918138414\n",
      "Epoch 3, Batch 10, Training Loss: 0.5022077560424805\n",
      "Epoch 3, Batch 20, Training Loss: 0.37571999430656433\n",
      "Epoch 3, Batch 30, Training Loss: 0.4680608808994293\n",
      "Epoch 3, Batch 40, Training Loss: 0.4124038517475128\n",
      "Epoch 3, Batch 50, Training Loss: 0.36330991983413696\n",
      "Epoch 3, Batch 60, Training Loss: 0.42039892077445984\n",
      "Epoch 3, Batch 70, Training Loss: 0.3580159544944763\n",
      "Epoch 3, Batch 80, Training Loss: 0.37682268023490906\n",
      "Epoch 3, Batch 90, Training Loss: 0.3993412256240845\n",
      "Epoch 3, Batch 100, Training Loss: 0.48920637369155884\n",
      "Epoch 3, Batch 110, Training Loss: 0.4663088023662567\n",
      "Epoch 3, Average Recall: 0.3860594695916192\n",
      "Epoch 4, Batch 10, Training Loss: 0.36140549182891846\n",
      "Epoch 4, Batch 20, Training Loss: 0.31537923216819763\n",
      "Epoch 4, Batch 30, Training Loss: 0.29425764083862305\n",
      "Epoch 4, Batch 40, Training Loss: 0.3033576011657715\n",
      "Epoch 4, Batch 50, Training Loss: 0.3550150990486145\n",
      "Epoch 4, Batch 60, Training Loss: 0.44540369510650635\n",
      "Epoch 4, Batch 70, Training Loss: 0.4230680465698242\n",
      "Epoch 4, Batch 80, Training Loss: 0.32675638794898987\n",
      "Epoch 4, Batch 90, Training Loss: 0.2968899607658386\n",
      "Epoch 4, Batch 100, Training Loss: 0.3026966452598572\n",
      "Epoch 4, Batch 110, Training Loss: 0.37568730115890503\n",
      "Epoch 4, Average Recall: 0.3873190011820331\n",
      "Epoch 5, Batch 10, Training Loss: 0.3307010531425476\n",
      "Epoch 5, Batch 20, Training Loss: 0.3077545762062073\n",
      "Epoch 5, Batch 30, Training Loss: 0.27284467220306396\n",
      "Epoch 5, Batch 40, Training Loss: 0.21592488884925842\n",
      "Epoch 5, Batch 50, Training Loss: 0.2640286982059479\n",
      "Epoch 5, Batch 60, Training Loss: 0.3241814970970154\n",
      "Epoch 5, Batch 70, Training Loss: 0.2756722569465637\n",
      "Epoch 5, Batch 80, Training Loss: 0.2834874391555786\n",
      "Epoch 5, Batch 90, Training Loss: 0.29338371753692627\n",
      "Epoch 5, Batch 100, Training Loss: 0.21209226548671722\n",
      "Epoch 5, Batch 110, Training Loss: 0.3534536063671112\n",
      "Epoch 5, Average Recall: 0.39179797953460344\n",
      "Epoch 6, Batch 10, Training Loss: 0.22874268889427185\n",
      "Epoch 6, Batch 20, Training Loss: 0.18627506494522095\n",
      "Epoch 6, Batch 30, Training Loss: 0.2794102728366852\n",
      "Epoch 6, Batch 40, Training Loss: 0.22991111874580383\n",
      "Epoch 6, Batch 50, Training Loss: 0.18242821097373962\n",
      "Epoch 6, Batch 60, Training Loss: 0.22655154764652252\n",
      "Epoch 6, Batch 70, Training Loss: 0.21269574761390686\n",
      "Epoch 6, Batch 80, Training Loss: 0.22689960896968842\n",
      "Epoch 6, Batch 90, Training Loss: 0.2293698787689209\n",
      "Epoch 6, Batch 100, Training Loss: 0.22417771816253662\n",
      "Epoch 6, Batch 110, Training Loss: 0.2512091398239136\n",
      "Epoch 6, Average Recall: 0.3915086276827516\n",
      "Epoch 7, Batch 10, Training Loss: 0.15039978921413422\n",
      "Epoch 7, Batch 20, Training Loss: 0.11858835816383362\n",
      "Epoch 7, Batch 30, Training Loss: 0.17653809487819672\n",
      "Epoch 7, Batch 40, Training Loss: 0.18941186368465424\n",
      "Epoch 7, Batch 50, Training Loss: 0.21065059304237366\n",
      "Epoch 7, Batch 60, Training Loss: 0.146269753575325\n",
      "Epoch 7, Batch 70, Training Loss: 0.11765344440937042\n",
      "Epoch 7, Batch 80, Training Loss: 0.2061062604188919\n",
      "Epoch 7, Batch 90, Training Loss: 0.1434156745672226\n",
      "Epoch 7, Batch 100, Training Loss: 0.2088686227798462\n",
      "Epoch 7, Batch 110, Training Loss: 0.24234819412231445\n",
      "Epoch 7, Average Recall: 0.3926319936958235\n",
      "Epoch 8, Batch 10, Training Loss: 0.14676988124847412\n",
      "Epoch 8, Batch 20, Training Loss: 0.1009795069694519\n",
      "Epoch 8, Batch 30, Training Loss: 0.15641355514526367\n",
      "Epoch 8, Batch 40, Training Loss: 0.1137278825044632\n",
      "Epoch 8, Batch 50, Training Loss: 0.136694073677063\n",
      "Epoch 8, Batch 60, Training Loss: 0.08781630545854568\n",
      "Epoch 8, Batch 70, Training Loss: 0.14074620604515076\n",
      "Epoch 8, Batch 80, Training Loss: 0.08910921216011047\n",
      "Epoch 8, Batch 90, Training Loss: 0.11285869777202606\n",
      "Epoch 8, Batch 100, Training Loss: 0.08290960639715195\n",
      "Epoch 8, Batch 110, Training Loss: 0.1441635936498642\n",
      "Epoch 8, Average Recall: 0.3923451768414221\n",
      "Epoch 9, Batch 10, Training Loss: 0.07508302479982376\n",
      "Epoch 9, Batch 20, Training Loss: 0.08821967244148254\n",
      "Epoch 9, Batch 30, Training Loss: 0.07459408044815063\n",
      "Epoch 9, Batch 40, Training Loss: 0.08062241226434708\n",
      "Epoch 9, Batch 50, Training Loss: 0.06555882096290588\n",
      "Epoch 9, Batch 60, Training Loss: 0.08418526500463486\n",
      "Epoch 9, Batch 70, Training Loss: 0.11002899706363678\n",
      "Epoch 9, Batch 80, Training Loss: 0.07853394001722336\n",
      "Epoch 9, Batch 90, Training Loss: 0.06622009724378586\n",
      "Epoch 9, Batch 100, Training Loss: 0.07394121587276459\n",
      "Epoch 9, Batch 110, Training Loss: 0.07983492314815521\n",
      "Epoch 9, Average Recall: 0.3913605114031429\n",
      "Epoch 10, Batch 10, Training Loss: 0.09680353105068207\n",
      "Epoch 10, Batch 20, Training Loss: 0.03246433660387993\n",
      "Epoch 10, Batch 30, Training Loss: 0.0666741207242012\n",
      "Epoch 10, Batch 40, Training Loss: 0.06259468197822571\n",
      "Epoch 10, Batch 50, Training Loss: 0.060195330530405045\n",
      "Epoch 10, Batch 60, Training Loss: 0.08561698347330093\n",
      "Epoch 10, Batch 70, Training Loss: 0.059223517775535583\n",
      "Epoch 10, Batch 80, Training Loss: 0.06264909356832504\n",
      "Epoch 10, Batch 90, Training Loss: 0.04941978305578232\n",
      "Epoch 10, Batch 100, Training Loss: 0.048172399401664734\n",
      "Epoch 10, Batch 110, Training Loss: 0.08186476677656174\n",
      "Epoch 10, Average Recall: 0.39266857008760947\n",
      "Epoch 1, Batch 10, Training Loss: 0.6361773610115051\n",
      "Epoch 1, Batch 20, Training Loss: 0.6798009872436523\n",
      "Epoch 1, Batch 30, Training Loss: 0.6616301536560059\n",
      "Epoch 1, Batch 40, Training Loss: 0.6452311277389526\n",
      "Epoch 1, Batch 50, Training Loss: 0.5795530080795288\n",
      "Epoch 1, Batch 60, Training Loss: 0.5744493007659912\n",
      "Epoch 1, Batch 70, Training Loss: 0.5945384502410889\n",
      "Epoch 1, Batch 80, Training Loss: 0.5704615712165833\n",
      "Epoch 1, Batch 90, Training Loss: 0.5566856861114502\n",
      "Epoch 1, Batch 100, Training Loss: 0.3674682080745697\n",
      "Epoch 1, Average Recall: 0.3722956730769231\n",
      "Epoch 2, Batch 10, Training Loss: 0.5680088400840759\n",
      "Epoch 2, Batch 20, Training Loss: 0.4458908438682556\n",
      "Epoch 2, Batch 30, Training Loss: 0.4917236566543579\n",
      "Epoch 2, Batch 40, Training Loss: 0.508908212184906\n",
      "Epoch 2, Batch 50, Training Loss: 0.5253857970237732\n",
      "Epoch 2, Batch 60, Training Loss: 0.4211239218711853\n",
      "Epoch 2, Batch 70, Training Loss: 0.40632981061935425\n",
      "Epoch 2, Batch 80, Training Loss: 0.4569980502128601\n",
      "Epoch 2, Batch 90, Training Loss: 0.5105622410774231\n",
      "Epoch 2, Batch 100, Training Loss: 0.3607669174671173\n",
      "Epoch 2, Average Recall: 0.3862179487179488\n",
      "Epoch 3, Batch 10, Training Loss: 0.4086390733718872\n",
      "Epoch 3, Batch 20, Training Loss: 0.4752977788448334\n",
      "Epoch 3, Batch 30, Training Loss: 0.39312586188316345\n",
      "Epoch 3, Batch 40, Training Loss: 0.3788229525089264\n",
      "Epoch 3, Batch 50, Training Loss: 0.38745975494384766\n",
      "Epoch 3, Batch 60, Training Loss: 0.3624832332134247\n",
      "Epoch 3, Batch 70, Training Loss: 0.39119961857795715\n",
      "Epoch 3, Batch 80, Training Loss: 0.37107381224632263\n",
      "Epoch 3, Batch 90, Training Loss: 0.37694820761680603\n",
      "Epoch 3, Batch 100, Training Loss: 0.4189226031303406\n",
      "Epoch 3, Average Recall: 0.4011418269230769\n",
      "Epoch 4, Batch 10, Training Loss: 0.37001216411590576\n",
      "Epoch 4, Batch 20, Training Loss: 0.35377296805381775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Batch 30, Training Loss: 0.3554750680923462\n",
      "Epoch 4, Batch 40, Training Loss: 0.24224138259887695\n",
      "Epoch 4, Batch 50, Training Loss: 0.3318103551864624\n",
      "Epoch 4, Batch 60, Training Loss: 0.3028776943683624\n",
      "Epoch 4, Batch 70, Training Loss: 0.2829110026359558\n",
      "Epoch 4, Batch 80, Training Loss: 0.31160834431648254\n",
      "Epoch 4, Batch 90, Training Loss: 0.30372291803359985\n",
      "Epoch 4, Batch 100, Training Loss: 0.4254316985607147\n",
      "Epoch 4, Average Recall: 0.4072516025641026\n",
      "Epoch 5, Batch 10, Training Loss: 0.24174433946609497\n",
      "Epoch 5, Batch 20, Training Loss: 0.2711203396320343\n",
      "Epoch 5, Batch 30, Training Loss: 0.20196621119976044\n",
      "Epoch 5, Batch 40, Training Loss: 0.34342801570892334\n",
      "Epoch 5, Batch 50, Training Loss: 0.3275623023509979\n",
      "Epoch 5, Batch 60, Training Loss: 0.2784552276134491\n",
      "Epoch 5, Batch 70, Training Loss: 0.25324884057044983\n",
      "Epoch 5, Batch 80, Training Loss: 0.25918343663215637\n",
      "Epoch 5, Batch 90, Training Loss: 0.38417255878448486\n",
      "Epoch 5, Batch 100, Training Loss: 0.35178789496421814\n",
      "Epoch 5, Average Recall: 0.4091546474358974\n",
      "Epoch 6, Batch 10, Training Loss: 0.31731295585632324\n",
      "Epoch 6, Batch 20, Training Loss: 0.20660407841205597\n",
      "Epoch 6, Batch 30, Training Loss: 0.23903250694274902\n",
      "Epoch 6, Batch 40, Training Loss: 0.24045135080814362\n",
      "Epoch 6, Batch 50, Training Loss: 0.2044879049062729\n",
      "Epoch 6, Batch 60, Training Loss: 0.2783384323120117\n",
      "Epoch 6, Batch 70, Training Loss: 0.23021678626537323\n",
      "Epoch 6, Batch 80, Training Loss: 0.18639391660690308\n",
      "Epoch 6, Batch 90, Training Loss: 0.21977302432060242\n",
      "Epoch 6, Batch 100, Training Loss: 0.28012147545814514\n",
      "Epoch 6, Average Recall: 0.4139623397435897\n",
      "Epoch 7, Batch 10, Training Loss: 0.16608524322509766\n",
      "Epoch 7, Batch 20, Training Loss: 0.17929968237876892\n",
      "Epoch 7, Batch 30, Training Loss: 0.1184481680393219\n",
      "Epoch 7, Batch 40, Training Loss: 0.15468010306358337\n",
      "Epoch 7, Batch 50, Training Loss: 0.10857311636209488\n",
      "Epoch 7, Batch 60, Training Loss: 0.1897290050983429\n",
      "Epoch 7, Batch 70, Training Loss: 0.18854843080043793\n",
      "Epoch 7, Batch 80, Training Loss: 0.1701219379901886\n",
      "Epoch 7, Batch 90, Training Loss: 0.18972541391849518\n",
      "Epoch 7, Batch 100, Training Loss: 0.20644105970859528\n",
      "Epoch 7, Average Recall: 0.414863782051282\n",
      "Epoch 8, Batch 10, Training Loss: 0.0978248119354248\n",
      "Epoch 8, Batch 20, Training Loss: 0.13908948004245758\n",
      "Epoch 8, Batch 30, Training Loss: 0.12358219921588898\n",
      "Epoch 8, Batch 40, Training Loss: 0.08398672938346863\n",
      "Epoch 8, Batch 50, Training Loss: 0.08747488260269165\n",
      "Epoch 8, Batch 60, Training Loss: 0.10044215619564056\n",
      "Epoch 8, Batch 70, Training Loss: 0.11913204193115234\n",
      "Epoch 8, Batch 80, Training Loss: 0.08674445003271103\n",
      "Epoch 8, Batch 90, Training Loss: 0.14172685146331787\n",
      "Epoch 8, Batch 100, Training Loss: 0.11690280586481094\n",
      "Epoch 8, Average Recall: 0.4150641025641026\n",
      "Epoch 9, Batch 10, Training Loss: 0.11690658330917358\n",
      "Epoch 9, Batch 20, Training Loss: 0.11183042824268341\n",
      "Epoch 9, Batch 30, Training Loss: 0.06830351054668427\n",
      "Epoch 9, Batch 40, Training Loss: 0.09818112105131149\n",
      "Epoch 9, Batch 50, Training Loss: 0.07828029990196228\n",
      "Epoch 9, Batch 60, Training Loss: 0.06558138877153397\n",
      "Epoch 9, Batch 70, Training Loss: 0.05836285650730133\n",
      "Epoch 9, Batch 80, Training Loss: 0.08559923619031906\n",
      "Epoch 9, Batch 90, Training Loss: 0.07821182906627655\n",
      "Epoch 9, Batch 100, Training Loss: 0.06158435344696045\n",
      "Epoch 9, Average Recall: 0.41346153846153844\n",
      "Epoch 10, Batch 10, Training Loss: 0.06361332535743713\n",
      "Epoch 10, Batch 20, Training Loss: 0.03707803413271904\n",
      "Epoch 10, Batch 30, Training Loss: 0.06465009599924088\n",
      "Epoch 10, Batch 40, Training Loss: 0.07334747165441513\n",
      "Epoch 10, Batch 50, Training Loss: 0.05756807327270508\n",
      "Epoch 10, Batch 60, Training Loss: 0.044031109660863876\n",
      "Epoch 10, Batch 70, Training Loss: 0.09109880030155182\n",
      "Epoch 10, Batch 80, Training Loss: 0.04499295353889465\n",
      "Epoch 10, Batch 90, Training Loss: 0.04454860836267471\n",
      "Epoch 10, Batch 100, Training Loss: 0.053801823407411575\n",
      "Epoch 10, Average Recall: 0.41546474358974356\n",
      "Epoch 1, Batch 10, Training Loss: 0.909793496131897\n",
      "Epoch 1, Batch 20, Training Loss: 0.6439065933227539\n",
      "Epoch 1, Batch 30, Training Loss: 0.8192821741104126\n",
      "Epoch 1, Batch 40, Training Loss: 0.6756870746612549\n",
      "Epoch 1, Batch 50, Training Loss: 0.7138369083404541\n",
      "Epoch 1, Batch 60, Training Loss: 0.6511263251304626\n",
      "Epoch 1, Batch 70, Training Loss: 0.6933276057243347\n",
      "Epoch 1, Batch 80, Training Loss: 0.7590606212615967\n",
      "Epoch 1, Batch 90, Training Loss: 0.6106350421905518\n",
      "Epoch 1, Batch 100, Training Loss: 0.6968408823013306\n",
      "Epoch 1, Batch 110, Training Loss: 0.6619555950164795\n",
      "Epoch 1, Batch 120, Training Loss: 0.6161201000213623\n",
      "Epoch 1, Batch 130, Training Loss: 0.5437510013580322\n",
      "Epoch 1, Batch 140, Training Loss: 0.5410575270652771\n",
      "Epoch 1, Batch 150, Training Loss: 0.7205145359039307\n",
      "Epoch 1, Batch 160, Training Loss: 0.6072165966033936\n",
      "Epoch 1, Batch 170, Training Loss: 0.5677083134651184\n",
      "Epoch 1, Batch 180, Training Loss: 0.5531342029571533\n",
      "Epoch 1, Batch 190, Training Loss: 0.5911858081817627\n",
      "Epoch 1, Batch 200, Training Loss: 0.40711817145347595\n",
      "Epoch 1, Batch 210, Training Loss: 0.4622090756893158\n",
      "Epoch 1, Batch 220, Training Loss: 0.5385856628417969\n",
      "Epoch 1, Batch 230, Training Loss: 0.524672269821167\n",
      "Epoch 1, Batch 240, Training Loss: 0.547316312789917\n",
      "Epoch 1, Batch 250, Training Loss: 0.5918264985084534\n",
      "Epoch 1, Batch 260, Training Loss: 0.5088858008384705\n",
      "Epoch 1, Batch 270, Training Loss: 0.48236116766929626\n",
      "Epoch 1, Average Recall: 0.384009573657049\n",
      "Epoch 2, Batch 10, Training Loss: 0.48970669507980347\n",
      "Epoch 2, Batch 20, Training Loss: 0.45428571105003357\n",
      "Epoch 2, Batch 30, Training Loss: 0.3913947343826294\n",
      "Epoch 2, Batch 40, Training Loss: 0.4109891653060913\n",
      "Epoch 2, Batch 50, Training Loss: 0.44140589237213135\n",
      "Epoch 2, Batch 60, Training Loss: 0.4156275689601898\n",
      "Epoch 2, Batch 70, Training Loss: 0.42300739884376526\n",
      "Epoch 2, Batch 80, Training Loss: 0.4664667248725891\n",
      "Epoch 2, Batch 90, Training Loss: 0.37362706661224365\n",
      "Epoch 2, Batch 100, Training Loss: 0.34807419776916504\n",
      "Epoch 2, Batch 110, Training Loss: 0.473217248916626\n",
      "Epoch 2, Batch 120, Training Loss: 0.44096100330352783\n",
      "Epoch 2, Batch 130, Training Loss: 0.43826204538345337\n",
      "Epoch 2, Batch 140, Training Loss: 0.4129863381385803\n",
      "Epoch 2, Batch 150, Training Loss: 0.40091556310653687\n",
      "Epoch 2, Batch 160, Training Loss: 0.36069849133491516\n",
      "Epoch 2, Batch 170, Training Loss: 0.44377583265304565\n",
      "Epoch 2, Batch 180, Training Loss: 0.5428078174591064\n",
      "Epoch 2, Batch 190, Training Loss: 0.33995357155799866\n",
      "Epoch 2, Batch 200, Training Loss: 0.45514413714408875\n",
      "Epoch 2, Batch 210, Training Loss: 0.4499661326408386\n",
      "Epoch 2, Batch 220, Training Loss: 0.4499260485172272\n",
      "Epoch 2, Batch 230, Training Loss: 0.37394315004348755\n",
      "Epoch 2, Batch 240, Training Loss: 0.5647344589233398\n",
      "Epoch 2, Batch 250, Training Loss: 0.3587476313114166\n",
      "Epoch 2, Batch 260, Training Loss: 0.48363184928894043\n",
      "Epoch 2, Batch 270, Training Loss: 0.4500844180583954\n",
      "Epoch 2, Average Recall: 0.3970755263312221\n",
      "Epoch 3, Batch 10, Training Loss: 0.46558696031570435\n",
      "Epoch 3, Batch 20, Training Loss: 0.34887805581092834\n",
      "Epoch 3, Batch 30, Training Loss: 0.3598010838031769\n",
      "Epoch 3, Batch 40, Training Loss: 0.2755504548549652\n",
      "Epoch 3, Batch 50, Training Loss: 0.3913838863372803\n",
      "Epoch 3, Batch 60, Training Loss: 0.30147549510002136\n",
      "Epoch 3, Batch 70, Training Loss: 0.36757078766822815\n",
      "Epoch 3, Batch 80, Training Loss: 0.33170023560523987\n",
      "Epoch 3, Batch 90, Training Loss: 0.37856680154800415\n",
      "Epoch 3, Batch 100, Training Loss: 0.2920328974723816\n",
      "Epoch 3, Batch 110, Training Loss: 0.3927026689052582\n",
      "Epoch 3, Batch 120, Training Loss: 0.3927912712097168\n",
      "Epoch 3, Batch 130, Training Loss: 0.3168618679046631\n",
      "Epoch 3, Batch 140, Training Loss: 0.2921808660030365\n",
      "Epoch 3, Batch 150, Training Loss: 0.4012979567050934\n",
      "Epoch 3, Batch 160, Training Loss: 0.284470796585083\n",
      "Epoch 3, Batch 170, Training Loss: 0.36928319931030273\n",
      "Epoch 3, Batch 180, Training Loss: 0.3450044095516205\n",
      "Epoch 3, Batch 190, Training Loss: 0.3257712125778198\n",
      "Epoch 3, Batch 200, Training Loss: 0.37941840291023254\n",
      "Epoch 3, Batch 210, Training Loss: 0.3500482141971588\n",
      "Epoch 3, Batch 220, Training Loss: 0.3629511594772339\n",
      "Epoch 3, Batch 230, Training Loss: 0.3309420347213745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Batch 240, Training Loss: 0.32409006357192993\n",
      "Epoch 3, Batch 250, Training Loss: 0.34889715909957886\n",
      "Epoch 3, Batch 260, Training Loss: 0.39005181193351746\n",
      "Epoch 3, Batch 270, Training Loss: 0.37744203209877014\n",
      "Epoch 3, Average Recall: 0.4012985280319024\n",
      "Epoch 4, Batch 10, Training Loss: 0.3163953423500061\n",
      "Epoch 4, Batch 20, Training Loss: 0.2840624749660492\n",
      "Epoch 4, Batch 30, Training Loss: 0.36833927035331726\n",
      "Epoch 4, Batch 40, Training Loss: 0.2737184166908264\n",
      "Epoch 4, Batch 50, Training Loss: 0.264776349067688\n",
      "Epoch 4, Batch 60, Training Loss: 0.4017985761165619\n",
      "Epoch 4, Batch 70, Training Loss: 0.3202695846557617\n",
      "Epoch 4, Batch 80, Training Loss: 0.2714241147041321\n",
      "Epoch 4, Batch 90, Training Loss: 0.2727409899234772\n",
      "Epoch 4, Batch 100, Training Loss: 0.2741881310939789\n",
      "Epoch 4, Batch 110, Training Loss: 0.30248063802719116\n",
      "Epoch 4, Batch 120, Training Loss: 0.33549046516418457\n",
      "Epoch 4, Batch 130, Training Loss: 0.2994648814201355\n",
      "Epoch 4, Batch 140, Training Loss: 0.22989685833454132\n",
      "Epoch 4, Batch 150, Training Loss: 0.31426742672920227\n",
      "Epoch 4, Batch 160, Training Loss: 0.2565099895000458\n",
      "Epoch 4, Batch 170, Training Loss: 0.24638010561466217\n",
      "Epoch 4, Batch 180, Training Loss: 0.26839783787727356\n",
      "Epoch 4, Batch 190, Training Loss: 0.3091701567173004\n",
      "Epoch 4, Batch 200, Training Loss: 0.41903847455978394\n",
      "Epoch 4, Batch 210, Training Loss: 0.27921873331069946\n",
      "Epoch 4, Batch 220, Training Loss: 0.2865196466445923\n",
      "Epoch 4, Batch 230, Training Loss: 0.3735341429710388\n",
      "Epoch 4, Batch 240, Training Loss: 0.3532058894634247\n",
      "Epoch 4, Batch 250, Training Loss: 0.23524945974349976\n",
      "Epoch 4, Batch 260, Training Loss: 0.21766692399978638\n",
      "Epoch 4, Batch 270, Training Loss: 0.27188390493392944\n",
      "Epoch 4, Average Recall: 0.4046315681444992\n",
      "Epoch 5, Batch 10, Training Loss: 0.19951532781124115\n",
      "Epoch 5, Batch 20, Training Loss: 0.305639386177063\n",
      "Epoch 5, Batch 30, Training Loss: 0.2638899087905884\n",
      "Epoch 5, Batch 40, Training Loss: 0.2204865962266922\n",
      "Epoch 5, Batch 50, Training Loss: 0.2987160086631775\n",
      "Epoch 5, Batch 60, Training Loss: 0.22363030910491943\n",
      "Epoch 5, Batch 70, Training Loss: 0.20982341468334198\n",
      "Epoch 5, Batch 80, Training Loss: 0.26122617721557617\n",
      "Epoch 5, Batch 90, Training Loss: 0.2837901711463928\n",
      "Epoch 5, Batch 100, Training Loss: 0.3118889331817627\n",
      "Epoch 5, Batch 110, Training Loss: 0.20699329674243927\n",
      "Epoch 5, Batch 120, Training Loss: 0.2394038289785385\n",
      "Epoch 5, Batch 130, Training Loss: 0.2215244174003601\n",
      "Epoch 5, Batch 140, Training Loss: 0.18079429864883423\n",
      "Epoch 5, Batch 150, Training Loss: 0.20510876178741455\n",
      "Epoch 5, Batch 160, Training Loss: 0.2552289664745331\n",
      "Epoch 5, Batch 170, Training Loss: 0.2674437165260315\n",
      "Epoch 5, Batch 180, Training Loss: 0.23345616459846497\n",
      "Epoch 5, Batch 190, Training Loss: 0.17439435422420502\n",
      "Epoch 5, Batch 200, Training Loss: 0.22341227531433105\n",
      "Epoch 5, Batch 210, Training Loss: 0.16223709285259247\n",
      "Epoch 5, Batch 220, Training Loss: 0.1944597065448761\n",
      "Epoch 5, Batch 230, Training Loss: 0.1736307144165039\n",
      "Epoch 5, Batch 240, Training Loss: 0.25516456365585327\n",
      "Epoch 5, Batch 250, Training Loss: 0.1530034840106964\n",
      "Epoch 5, Batch 260, Training Loss: 0.18720969557762146\n",
      "Epoch 5, Batch 270, Training Loss: 0.25184497237205505\n",
      "Epoch 5, Average Recall: 0.40384635966455545\n",
      "Epoch 6, Batch 10, Training Loss: 0.1408209651708603\n",
      "Epoch 6, Batch 20, Training Loss: 0.12787167727947235\n",
      "Epoch 6, Batch 30, Training Loss: 0.201759934425354\n",
      "Epoch 6, Batch 40, Training Loss: 0.14165927469730377\n",
      "Epoch 6, Batch 50, Training Loss: 0.18263262510299683\n",
      "Epoch 6, Batch 60, Training Loss: 0.18098056316375732\n",
      "Epoch 6, Batch 70, Training Loss: 0.16144685447216034\n",
      "Epoch 6, Batch 80, Training Loss: 0.15524528920650482\n",
      "Epoch 6, Batch 90, Training Loss: 0.19730491936206818\n",
      "Epoch 6, Batch 100, Training Loss: 0.2640632390975952\n",
      "Epoch 6, Batch 110, Training Loss: 0.22189193964004517\n",
      "Epoch 6, Batch 120, Training Loss: 0.149102583527565\n",
      "Epoch 6, Batch 130, Training Loss: 0.15329615771770477\n",
      "Epoch 6, Batch 140, Training Loss: 0.1687173992395401\n",
      "Epoch 6, Batch 150, Training Loss: 0.0956173986196518\n",
      "Epoch 6, Batch 160, Training Loss: 0.17132499814033508\n",
      "Epoch 6, Batch 170, Training Loss: 0.18716324865818024\n",
      "Epoch 6, Batch 180, Training Loss: 0.14413167536258698\n",
      "Epoch 6, Batch 190, Training Loss: 0.20742203295230865\n",
      "Epoch 6, Batch 200, Training Loss: 0.16083039343357086\n",
      "Epoch 6, Batch 210, Training Loss: 0.17360138893127441\n",
      "Epoch 6, Batch 220, Training Loss: 0.12306329607963562\n",
      "Epoch 6, Batch 230, Training Loss: 0.17489828169345856\n",
      "Epoch 6, Batch 240, Training Loss: 0.15919117629528046\n",
      "Epoch 6, Batch 250, Training Loss: 0.09268415719270706\n",
      "Epoch 6, Batch 260, Training Loss: 0.19231878221035004\n",
      "Epoch 6, Batch 270, Training Loss: 0.15513065457344055\n",
      "Epoch 6, Average Recall: 0.4024185579404176\n",
      "Epoch 7, Batch 10, Training Loss: 0.0850808322429657\n",
      "Epoch 7, Batch 20, Training Loss: 0.10846377909183502\n",
      "Epoch 7, Batch 30, Training Loss: 0.08676750957965851\n",
      "Epoch 7, Batch 40, Training Loss: 0.10414385795593262\n",
      "Epoch 7, Batch 50, Training Loss: 0.11438465118408203\n",
      "Epoch 7, Batch 60, Training Loss: 0.08928917348384857\n",
      "Epoch 7, Batch 70, Training Loss: 0.07239411026239395\n",
      "Epoch 7, Batch 80, Training Loss: 0.13086211681365967\n",
      "Epoch 7, Batch 90, Training Loss: 0.06504464149475098\n",
      "Epoch 7, Batch 100, Training Loss: 0.16442184150218964\n",
      "Epoch 7, Batch 110, Training Loss: 0.07672098278999329\n",
      "Epoch 7, Batch 120, Training Loss: 0.14852987229824066\n",
      "Epoch 7, Batch 130, Training Loss: 0.07331594079732895\n",
      "Epoch 7, Batch 140, Training Loss: 0.1633453369140625\n",
      "Epoch 7, Batch 150, Training Loss: 0.08610228449106216\n",
      "Epoch 7, Batch 160, Training Loss: 0.11562898755073547\n",
      "Epoch 7, Batch 170, Training Loss: 0.11777019500732422\n",
      "Epoch 7, Batch 180, Training Loss: 0.11978249996900558\n",
      "Epoch 7, Batch 190, Training Loss: 0.17175132036209106\n",
      "Epoch 7, Batch 200, Training Loss: 0.11732814460992813\n",
      "Epoch 7, Batch 210, Training Loss: 0.09684573113918304\n",
      "Epoch 7, Batch 220, Training Loss: 0.11397625505924225\n",
      "Epoch 7, Batch 230, Training Loss: 0.07239092141389847\n",
      "Epoch 7, Batch 240, Training Loss: 0.09679924696683884\n",
      "Epoch 7, Batch 250, Training Loss: 0.10035999864339828\n",
      "Epoch 7, Batch 260, Training Loss: 0.1337117701768875\n",
      "Epoch 7, Batch 270, Training Loss: 0.08278296887874603\n",
      "Epoch 7, Average Recall: 0.40286905494956604\n",
      "Epoch 8, Batch 10, Training Loss: 0.037664078176021576\n",
      "Epoch 8, Batch 20, Training Loss: 0.07762011140584946\n",
      "Epoch 8, Batch 30, Training Loss: 0.051115624606609344\n",
      "Epoch 8, Batch 40, Training Loss: 0.052887409925460815\n",
      "Epoch 8, Batch 50, Training Loss: 0.07061397284269333\n",
      "Epoch 8, Batch 60, Training Loss: 0.08007944375276566\n",
      "Epoch 8, Batch 70, Training Loss: 0.09426774084568024\n",
      "Epoch 8, Batch 80, Training Loss: 0.08526626229286194\n",
      "Epoch 8, Batch 90, Training Loss: 0.06427881866693497\n",
      "Epoch 8, Batch 100, Training Loss: 0.06537885218858719\n",
      "Epoch 8, Batch 110, Training Loss: 0.10293081402778625\n",
      "Epoch 8, Batch 120, Training Loss: 0.05557490140199661\n",
      "Epoch 8, Batch 130, Training Loss: 0.06322849541902542\n",
      "Epoch 8, Batch 140, Training Loss: 0.06514675170183182\n",
      "Epoch 8, Batch 150, Training Loss: 0.1837199479341507\n",
      "Epoch 8, Batch 160, Training Loss: 0.057053182274103165\n",
      "Epoch 8, Batch 170, Training Loss: 0.06454496830701828\n",
      "Epoch 8, Batch 180, Training Loss: 0.03795577213168144\n",
      "Epoch 8, Batch 190, Training Loss: 0.045148637145757675\n",
      "Epoch 8, Batch 200, Training Loss: 0.04759795218706131\n",
      "Epoch 8, Batch 210, Training Loss: 0.0455196313560009\n",
      "Epoch 8, Batch 220, Training Loss: 0.05931684374809265\n",
      "Epoch 8, Batch 230, Training Loss: 0.0731068029999733\n",
      "Epoch 8, Batch 240, Training Loss: 0.05192719027400017\n",
      "Epoch 8, Batch 250, Training Loss: 0.065329410135746\n",
      "Epoch 8, Batch 260, Training Loss: 0.11046900600194931\n",
      "Epoch 8, Batch 270, Training Loss: 0.050195422023534775\n",
      "Epoch 8, Average Recall: 0.40246485016420364\n",
      "Epoch 9, Batch 10, Training Loss: 0.03822009637951851\n",
      "Epoch 9, Batch 20, Training Loss: 0.06292936950922012\n",
      "Epoch 9, Batch 30, Training Loss: 0.06108127161860466\n",
      "Epoch 9, Batch 40, Training Loss: 0.02849646285176277\n",
      "Epoch 9, Batch 50, Training Loss: 0.05238230153918266\n",
      "Epoch 9, Batch 60, Training Loss: 0.03013344667851925\n",
      "Epoch 9, Batch 70, Training Loss: 0.029335785657167435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Batch 80, Training Loss: 0.05887305736541748\n",
      "Epoch 9, Batch 90, Training Loss: 0.050336845219135284\n",
      "Epoch 9, Batch 100, Training Loss: 0.03883109986782074\n",
      "Epoch 9, Batch 110, Training Loss: 0.05147847160696983\n",
      "Epoch 9, Batch 120, Training Loss: 0.030235782265663147\n",
      "Epoch 9, Batch 130, Training Loss: 0.0377049520611763\n",
      "Epoch 9, Batch 140, Training Loss: 0.06961099058389664\n",
      "Epoch 9, Batch 150, Training Loss: 0.04222485423088074\n",
      "Epoch 9, Batch 160, Training Loss: 0.033776216208934784\n",
      "Epoch 9, Batch 170, Training Loss: 0.05822237208485603\n",
      "Epoch 9, Batch 180, Training Loss: 0.02958717942237854\n",
      "Epoch 9, Batch 190, Training Loss: 0.0752926766872406\n",
      "Epoch 9, Batch 200, Training Loss: 0.0662352442741394\n",
      "Epoch 9, Batch 210, Training Loss: 0.06283382326364517\n",
      "Epoch 9, Batch 220, Training Loss: 0.06664320826530457\n",
      "Epoch 9, Batch 230, Training Loss: 0.036932073533535004\n",
      "Epoch 9, Batch 240, Training Loss: 0.04118943586945534\n",
      "Epoch 9, Batch 250, Training Loss: 0.03844171389937401\n",
      "Epoch 9, Batch 260, Training Loss: 0.027298703789711\n",
      "Epoch 9, Batch 270, Training Loss: 0.03736428916454315\n",
      "Epoch 9, Average Recall: 0.4029997947454844\n",
      "Epoch 10, Batch 10, Training Loss: 0.03466536104679108\n",
      "Epoch 10, Batch 20, Training Loss: 0.025266384705901146\n",
      "Epoch 10, Batch 30, Training Loss: 0.03646930307149887\n",
      "Epoch 10, Batch 40, Training Loss: 0.03528076410293579\n",
      "Epoch 10, Batch 50, Training Loss: 0.03799616917967796\n",
      "Epoch 10, Batch 60, Training Loss: 0.04112865403294563\n",
      "Epoch 10, Batch 70, Training Loss: 0.02616666816174984\n",
      "Epoch 10, Batch 80, Training Loss: 0.018447455018758774\n",
      "Epoch 10, Batch 90, Training Loss: 0.024351816624403\n",
      "Epoch 10, Batch 100, Training Loss: 0.012630344368517399\n",
      "Epoch 10, Batch 110, Training Loss: 0.027620961889624596\n",
      "Epoch 10, Batch 120, Training Loss: 0.019450584426522255\n",
      "Epoch 10, Batch 130, Training Loss: 0.02178540825843811\n",
      "Epoch 10, Batch 140, Training Loss: 0.046422623097896576\n",
      "Epoch 10, Batch 150, Training Loss: 0.050621211528778076\n",
      "Epoch 10, Batch 160, Training Loss: 0.02459740824997425\n",
      "Epoch 10, Batch 170, Training Loss: 0.027872059494256973\n",
      "Epoch 10, Batch 180, Training Loss: 0.016951212659478188\n",
      "Epoch 10, Batch 190, Training Loss: 0.018973050639033318\n",
      "Epoch 10, Batch 200, Training Loss: 0.023274289444088936\n",
      "Epoch 10, Batch 210, Training Loss: 0.04872091859579086\n",
      "Epoch 10, Batch 220, Training Loss: 0.030935395509004593\n",
      "Epoch 10, Batch 230, Training Loss: 0.01474984921514988\n",
      "Epoch 10, Batch 240, Training Loss: 0.041823916137218475\n",
      "Epoch 10, Batch 250, Training Loss: 0.01823130063712597\n",
      "Epoch 10, Batch 260, Training Loss: 0.029807770624756813\n",
      "Epoch 10, Batch 270, Training Loss: 0.01826055347919464\n",
      "Epoch 10, Average Recall: 0.40134482025568846\n",
      "Epoch 1, Batch 10, Training Loss: 0.6685345768928528\n",
      "Epoch 1, Batch 20, Training Loss: 0.6302250027656555\n",
      "Epoch 1, Batch 30, Training Loss: 0.6727283000946045\n",
      "Epoch 1, Batch 40, Training Loss: 0.5895596742630005\n",
      "Epoch 1, Batch 50, Training Loss: 0.5917435884475708\n",
      "Epoch 1, Batch 60, Training Loss: 0.5578811168670654\n",
      "Epoch 1, Batch 70, Training Loss: 0.5650904178619385\n",
      "Epoch 1, Batch 80, Training Loss: 0.5950982570648193\n",
      "Epoch 1, Batch 90, Training Loss: 0.5167105197906494\n",
      "Epoch 1, Batch 100, Training Loss: 0.6091039776802063\n",
      "Epoch 1, Batch 110, Training Loss: 0.5846013426780701\n",
      "Epoch 1, Batch 120, Training Loss: 0.5626379251480103\n",
      "Epoch 1, Batch 130, Training Loss: 0.5342458486557007\n",
      "Epoch 1, Batch 140, Training Loss: 0.5633324384689331\n",
      "Epoch 1, Batch 150, Training Loss: 0.5076207518577576\n",
      "Epoch 1, Batch 160, Training Loss: 0.4952602982521057\n",
      "Epoch 1, Batch 170, Training Loss: 0.4757647216320038\n",
      "Epoch 1, Batch 180, Training Loss: 0.553787350654602\n",
      "Epoch 1, Batch 190, Training Loss: 0.658493161201477\n",
      "Epoch 1, Batch 200, Training Loss: 0.5258862972259521\n",
      "Epoch 1, Batch 210, Training Loss: 0.5066885352134705\n",
      "Epoch 1, Average Recall: 0.3756555451127819\n",
      "Epoch 2, Batch 10, Training Loss: 0.5191699266433716\n",
      "Epoch 2, Batch 20, Training Loss: 0.39826154708862305\n",
      "Epoch 2, Batch 30, Training Loss: 0.4071870446205139\n",
      "Epoch 2, Batch 40, Training Loss: 0.408963680267334\n",
      "Epoch 2, Batch 50, Training Loss: 0.4624355137348175\n",
      "Epoch 2, Batch 60, Training Loss: 0.5277538299560547\n",
      "Epoch 2, Batch 70, Training Loss: 0.3646310269832611\n",
      "Epoch 2, Batch 80, Training Loss: 0.4827686548233032\n",
      "Epoch 2, Batch 90, Training Loss: 0.46496105194091797\n",
      "Epoch 2, Batch 100, Training Loss: 0.46180295944213867\n",
      "Epoch 2, Batch 110, Training Loss: 0.47087106108665466\n",
      "Epoch 2, Batch 120, Training Loss: 0.35900264978408813\n",
      "Epoch 2, Batch 130, Training Loss: 0.478397011756897\n",
      "Epoch 2, Batch 140, Training Loss: 0.44831717014312744\n",
      "Epoch 2, Batch 150, Training Loss: 0.3741056025028229\n",
      "Epoch 2, Batch 160, Training Loss: 0.4749503433704376\n",
      "Epoch 2, Batch 170, Training Loss: 0.40304017066955566\n",
      "Epoch 2, Batch 180, Training Loss: 0.4988565444946289\n",
      "Epoch 2, Batch 190, Training Loss: 0.43979424238204956\n",
      "Epoch 2, Batch 200, Training Loss: 0.48399239778518677\n",
      "Epoch 2, Batch 210, Training Loss: 0.4520378112792969\n",
      "Epoch 2, Average Recall: 0.3828234649122807\n",
      "Epoch 3, Batch 10, Training Loss: 0.34863221645355225\n",
      "Epoch 3, Batch 20, Training Loss: 0.3880642354488373\n",
      "Epoch 3, Batch 30, Training Loss: 0.3251846134662628\n",
      "Epoch 3, Batch 40, Training Loss: 0.42738059163093567\n",
      "Epoch 3, Batch 50, Training Loss: 0.43469715118408203\n",
      "Epoch 3, Batch 60, Training Loss: 0.42066094279289246\n",
      "Epoch 3, Batch 70, Training Loss: 0.34524503350257874\n",
      "Epoch 3, Batch 80, Training Loss: 0.38368305563926697\n",
      "Epoch 3, Batch 90, Training Loss: 0.3715023398399353\n",
      "Epoch 3, Batch 100, Training Loss: 0.4285831153392792\n",
      "Epoch 3, Batch 110, Training Loss: 0.39431536197662354\n",
      "Epoch 3, Batch 120, Training Loss: 0.2911427915096283\n",
      "Epoch 3, Batch 130, Training Loss: 0.3342171907424927\n",
      "Epoch 3, Batch 140, Training Loss: 0.324651837348938\n",
      "Epoch 3, Batch 150, Training Loss: 0.27755454182624817\n",
      "Epoch 3, Batch 160, Training Loss: 0.3871511220932007\n",
      "Epoch 3, Batch 170, Training Loss: 0.3611817955970764\n",
      "Epoch 3, Batch 180, Training Loss: 0.4342460036277771\n",
      "Epoch 3, Batch 190, Training Loss: 0.36381399631500244\n",
      "Epoch 3, Batch 200, Training Loss: 0.3078558146953583\n",
      "Epoch 3, Batch 210, Training Loss: 0.37175989151000977\n",
      "Epoch 3, Average Recall: 0.38620927318295734\n",
      "Epoch 4, Batch 10, Training Loss: 0.37938541173934937\n",
      "Epoch 4, Batch 20, Training Loss: 0.28281545639038086\n",
      "Epoch 4, Batch 30, Training Loss: 0.241695374250412\n",
      "Epoch 4, Batch 40, Training Loss: 0.39872679114341736\n",
      "Epoch 4, Batch 50, Training Loss: 0.2610272169113159\n",
      "Epoch 4, Batch 60, Training Loss: 0.23555080592632294\n",
      "Epoch 4, Batch 70, Training Loss: 0.3550010919570923\n",
      "Epoch 4, Batch 80, Training Loss: 0.28134357929229736\n",
      "Epoch 4, Batch 90, Training Loss: 0.2797085642814636\n",
      "Epoch 4, Batch 100, Training Loss: 0.326433926820755\n",
      "Epoch 4, Batch 110, Training Loss: 0.2795564532279968\n",
      "Epoch 4, Batch 120, Training Loss: 0.2699197828769684\n",
      "Epoch 4, Batch 130, Training Loss: 0.2925927937030792\n",
      "Epoch 4, Batch 140, Training Loss: 0.27723509073257446\n",
      "Epoch 4, Batch 150, Training Loss: 0.25214147567749023\n",
      "Epoch 4, Batch 160, Training Loss: 0.2604278326034546\n",
      "Epoch 4, Batch 170, Training Loss: 0.29822179675102234\n",
      "Epoch 4, Batch 180, Training Loss: 0.2353549599647522\n",
      "Epoch 4, Batch 190, Training Loss: 0.2836647033691406\n",
      "Epoch 4, Batch 200, Training Loss: 0.2801184356212616\n",
      "Epoch 4, Batch 210, Training Loss: 0.30734387040138245\n",
      "Epoch 4, Average Recall: 0.3857597117794486\n",
      "Epoch 5, Batch 10, Training Loss: 0.2961396872997284\n",
      "Epoch 5, Batch 20, Training Loss: 0.2509666681289673\n",
      "Epoch 5, Batch 30, Training Loss: 0.19594573974609375\n",
      "Epoch 5, Batch 40, Training Loss: 0.288908988237381\n",
      "Epoch 5, Batch 50, Training Loss: 0.30888426303863525\n",
      "Epoch 5, Batch 60, Training Loss: 0.2381807267665863\n",
      "Epoch 5, Batch 70, Training Loss: 0.27980345487594604\n",
      "Epoch 5, Batch 80, Training Loss: 0.2062048763036728\n",
      "Epoch 5, Batch 90, Training Loss: 0.23561826348304749\n",
      "Epoch 5, Batch 100, Training Loss: 0.29133766889572144\n",
      "Epoch 5, Batch 110, Training Loss: 0.21837273240089417\n",
      "Epoch 5, Batch 120, Training Loss: 0.20243871212005615\n",
      "Epoch 5, Batch 130, Training Loss: 0.2676387429237366\n",
      "Epoch 5, Batch 140, Training Loss: 0.21638242900371552\n",
      "Epoch 5, Batch 150, Training Loss: 0.1620253473520279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Batch 160, Training Loss: 0.28219932317733765\n",
      "Epoch 5, Batch 170, Training Loss: 0.161311537027359\n",
      "Epoch 5, Batch 180, Training Loss: 0.2506256401538849\n",
      "Epoch 5, Batch 190, Training Loss: 0.25798314809799194\n",
      "Epoch 5, Batch 200, Training Loss: 0.26434579491615295\n",
      "Epoch 5, Batch 210, Training Loss: 0.2656509280204773\n",
      "Epoch 5, Average Recall: 0.38799185463659147\n",
      "Epoch 6, Batch 10, Training Loss: 0.3041685223579407\n",
      "Epoch 6, Batch 20, Training Loss: 0.1893516182899475\n",
      "Epoch 6, Batch 30, Training Loss: 0.23082883656024933\n",
      "Epoch 6, Batch 40, Training Loss: 0.1848507523536682\n",
      "Epoch 6, Batch 50, Training Loss: 0.1671205312013626\n",
      "Epoch 6, Batch 60, Training Loss: 0.16614185273647308\n",
      "Epoch 6, Batch 70, Training Loss: 0.21429775655269623\n",
      "Epoch 6, Batch 80, Training Loss: 0.1672828495502472\n",
      "Epoch 6, Batch 90, Training Loss: 0.19265642762184143\n",
      "Epoch 6, Batch 100, Training Loss: 0.1375107765197754\n",
      "Epoch 6, Batch 110, Training Loss: 0.14048129320144653\n",
      "Epoch 6, Batch 120, Training Loss: 0.18458539247512817\n",
      "Epoch 6, Batch 130, Training Loss: 0.1382911205291748\n",
      "Epoch 6, Batch 140, Training Loss: 0.13860146701335907\n",
      "Epoch 6, Batch 150, Training Loss: 0.12986361980438232\n",
      "Epoch 6, Batch 160, Training Loss: 0.2199104279279709\n",
      "Epoch 6, Batch 170, Training Loss: 0.14366941154003143\n",
      "Epoch 6, Batch 180, Training Loss: 0.18942520022392273\n",
      "Epoch 6, Batch 190, Training Loss: 0.20610547065734863\n",
      "Epoch 6, Batch 200, Training Loss: 0.15663696825504303\n",
      "Epoch 6, Batch 210, Training Loss: 0.16104698181152344\n",
      "Epoch 6, Average Recall: 0.38699326441102755\n",
      "Epoch 7, Batch 10, Training Loss: 0.141020268201828\n",
      "Epoch 7, Batch 20, Training Loss: 0.13881206512451172\n",
      "Epoch 7, Batch 30, Training Loss: 0.10793669521808624\n",
      "Epoch 7, Batch 40, Training Loss: 0.13262158632278442\n",
      "Epoch 7, Batch 50, Training Loss: 0.142661452293396\n",
      "Epoch 7, Batch 60, Training Loss: 0.12799663841724396\n",
      "Epoch 7, Batch 70, Training Loss: 0.132058247923851\n",
      "Epoch 7, Batch 80, Training Loss: 0.13485901057720184\n",
      "Epoch 7, Batch 90, Training Loss: 0.09179375320672989\n",
      "Epoch 7, Batch 100, Training Loss: 0.14049892127513885\n",
      "Epoch 7, Batch 110, Training Loss: 0.10343039035797119\n",
      "Epoch 7, Batch 120, Training Loss: 0.14217759668827057\n",
      "Epoch 7, Batch 130, Training Loss: 0.1435190588235855\n",
      "Epoch 7, Batch 140, Training Loss: 0.10546997934579849\n",
      "Epoch 7, Batch 150, Training Loss: 0.09244942665100098\n",
      "Epoch 7, Batch 160, Training Loss: 0.10830598324537277\n",
      "Epoch 7, Batch 170, Training Loss: 0.08864087611436844\n",
      "Epoch 7, Batch 180, Training Loss: 0.11266762018203735\n",
      "Epoch 7, Batch 190, Training Loss: 0.08281462639570236\n",
      "Epoch 7, Batch 200, Training Loss: 0.12660042941570282\n",
      "Epoch 7, Batch 210, Training Loss: 0.11901231110095978\n",
      "Epoch 7, Average Recall: 0.3887359022556391\n",
      "Epoch 8, Batch 10, Training Loss: 0.07143062353134155\n",
      "Epoch 8, Batch 20, Training Loss: 0.08367349207401276\n",
      "Epoch 8, Batch 30, Training Loss: 0.04255398362874985\n",
      "Epoch 8, Batch 40, Training Loss: 0.0663575753569603\n",
      "Epoch 8, Batch 50, Training Loss: 0.04511275514960289\n",
      "Epoch 8, Batch 60, Training Loss: 0.0799567922949791\n",
      "Epoch 8, Batch 70, Training Loss: 0.06398129463195801\n",
      "Epoch 8, Batch 80, Training Loss: 0.08342423290014267\n",
      "Epoch 8, Batch 90, Training Loss: 0.06762982904911041\n",
      "Epoch 8, Batch 100, Training Loss: 0.0628872960805893\n",
      "Epoch 8, Batch 110, Training Loss: 0.10957905650138855\n",
      "Epoch 8, Batch 120, Training Loss: 0.18895044922828674\n",
      "Epoch 8, Batch 130, Training Loss: 0.061437007039785385\n",
      "Epoch 8, Batch 140, Training Loss: 0.060847461223602295\n",
      "Epoch 8, Batch 150, Training Loss: 0.07154005765914917\n",
      "Epoch 8, Batch 160, Training Loss: 0.0779871866106987\n",
      "Epoch 8, Batch 170, Training Loss: 0.06996893882751465\n",
      "Epoch 8, Batch 180, Training Loss: 0.04554948955774307\n",
      "Epoch 8, Batch 190, Training Loss: 0.05229518190026283\n",
      "Epoch 8, Batch 200, Training Loss: 0.1561012715101242\n",
      "Epoch 8, Batch 210, Training Loss: 0.16007748246192932\n",
      "Epoch 8, Average Recall: 0.38783599624060155\n",
      "Epoch 9, Batch 10, Training Loss: 0.035658735781908035\n",
      "Epoch 9, Batch 20, Training Loss: 0.04652535915374756\n",
      "Epoch 9, Batch 30, Training Loss: 0.07104311138391495\n",
      "Epoch 9, Batch 40, Training Loss: 0.052983373403549194\n",
      "Epoch 9, Batch 50, Training Loss: 0.03679300472140312\n",
      "Epoch 9, Batch 60, Training Loss: 0.037784237414598465\n",
      "Epoch 9, Batch 70, Training Loss: 0.06837043166160583\n",
      "Epoch 9, Batch 80, Training Loss: 0.029643714427947998\n",
      "Epoch 9, Batch 90, Training Loss: 0.03210943937301636\n",
      "Epoch 9, Batch 100, Training Loss: 0.04185250774025917\n",
      "Epoch 9, Batch 110, Training Loss: 0.03573421761393547\n",
      "Epoch 9, Batch 120, Training Loss: 0.09650193154811859\n",
      "Epoch 9, Batch 130, Training Loss: 0.039285194128751755\n",
      "Epoch 9, Batch 140, Training Loss: 0.06526295095682144\n",
      "Epoch 9, Batch 150, Training Loss: 0.045729003846645355\n",
      "Epoch 9, Batch 160, Training Loss: 0.05339604988694191\n",
      "Epoch 9, Batch 170, Training Loss: 0.06011277437210083\n",
      "Epoch 9, Batch 180, Training Loss: 0.08689095079898834\n",
      "Epoch 9, Batch 190, Training Loss: 0.03678316995501518\n",
      "Epoch 9, Batch 200, Training Loss: 0.07374300062656403\n",
      "Epoch 9, Batch 210, Training Loss: 0.054590437561273575\n",
      "Epoch 9, Average Recall: 0.38611293859649126\n",
      "Epoch 10, Batch 10, Training Loss: 0.03226657584309578\n",
      "Epoch 10, Batch 20, Training Loss: 0.03209300711750984\n",
      "Epoch 10, Batch 30, Training Loss: 0.029818203300237656\n",
      "Epoch 10, Batch 40, Training Loss: 0.03253328427672386\n",
      "Epoch 10, Batch 50, Training Loss: 0.04740944504737854\n",
      "Epoch 10, Batch 60, Training Loss: 0.029299601912498474\n",
      "Epoch 10, Batch 70, Training Loss: 0.03180877864360809\n",
      "Epoch 10, Batch 80, Training Loss: 0.07209005206823349\n",
      "Epoch 10, Batch 90, Training Loss: 0.03834174573421478\n",
      "Epoch 10, Batch 100, Training Loss: 0.12390308082103729\n",
      "Epoch 10, Batch 110, Training Loss: 0.024584518745541573\n",
      "Epoch 10, Batch 120, Training Loss: 0.0189634021371603\n",
      "Epoch 10, Batch 130, Training Loss: 0.02308320440351963\n",
      "Epoch 10, Batch 140, Training Loss: 0.03504730388522148\n",
      "Epoch 10, Batch 150, Training Loss: 0.03176571801304817\n",
      "Epoch 10, Batch 160, Training Loss: 0.033052798360586166\n",
      "Epoch 10, Batch 170, Training Loss: 0.026871684938669205\n",
      "Epoch 10, Batch 180, Training Loss: 0.02683207392692566\n",
      "Epoch 10, Batch 190, Training Loss: 0.016215432435274124\n",
      "Epoch 10, Batch 200, Training Loss: 0.03779861330986023\n",
      "Epoch 10, Batch 210, Training Loss: 0.02704797312617302\n",
      "Epoch 10, Average Recall: 0.38513236215538843\n",
      "Epoch 1, Batch 10, Training Loss: 0.687168538570404\n",
      "Epoch 1, Batch 20, Training Loss: 0.6493215560913086\n",
      "Epoch 1, Batch 30, Training Loss: 0.6014165282249451\n",
      "Epoch 1, Batch 40, Training Loss: 0.6182535886764526\n",
      "Epoch 1, Batch 50, Training Loss: 0.5690630078315735\n",
      "Epoch 1, Batch 60, Training Loss: 0.5901763439178467\n",
      "Epoch 1, Batch 70, Training Loss: 0.5232606530189514\n",
      "Epoch 1, Batch 80, Training Loss: 0.6080010533332825\n",
      "Epoch 1, Batch 90, Training Loss: 0.6466071605682373\n",
      "Epoch 1, Batch 100, Training Loss: 0.5604919195175171\n",
      "Epoch 1, Batch 110, Training Loss: 0.59828120470047\n",
      "Epoch 1, Batch 120, Training Loss: 0.5189056396484375\n",
      "Epoch 1, Batch 130, Training Loss: 0.5631608963012695\n",
      "Epoch 1, Batch 140, Training Loss: 0.6329050064086914\n",
      "Epoch 1, Batch 150, Training Loss: 0.6205496788024902\n",
      "Epoch 1, Batch 160, Training Loss: 0.5503835082054138\n",
      "Epoch 1, Batch 170, Training Loss: 0.4633147418498993\n",
      "Epoch 1, Batch 180, Training Loss: 0.5110001564025879\n",
      "Epoch 1, Batch 190, Training Loss: 0.5047104954719543\n",
      "Epoch 1, Batch 200, Training Loss: 0.4644668400287628\n",
      "Epoch 1, Batch 210, Training Loss: 0.44461339712142944\n",
      "Epoch 1, Batch 220, Training Loss: 0.4049198031425476\n",
      "Epoch 1, Batch 230, Training Loss: 0.5158718228340149\n",
      "Epoch 1, Batch 240, Training Loss: 0.484535813331604\n",
      "Epoch 1, Batch 250, Training Loss: 0.5237897634506226\n",
      "Epoch 1, Batch 260, Training Loss: 0.6460084915161133\n",
      "Epoch 1, Average Recall: 0.39450996633442953\n",
      "Epoch 2, Batch 10, Training Loss: 0.4784923195838928\n",
      "Epoch 2, Batch 20, Training Loss: 0.37592726945877075\n",
      "Epoch 2, Batch 30, Training Loss: 0.3646205961704254\n",
      "Epoch 2, Batch 40, Training Loss: 0.4608636796474457\n",
      "Epoch 2, Batch 50, Training Loss: 0.3613116443157196\n",
      "Epoch 2, Batch 60, Training Loss: 0.3529844284057617\n",
      "Epoch 2, Batch 70, Training Loss: 0.3304227888584137\n",
      "Epoch 2, Batch 80, Training Loss: 0.39013731479644775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 90, Training Loss: 0.4143853187561035\n",
      "Epoch 2, Batch 100, Training Loss: 0.3696199059486389\n",
      "Epoch 2, Batch 110, Training Loss: 0.4488688111305237\n",
      "Epoch 2, Batch 120, Training Loss: 0.43781620264053345\n",
      "Epoch 2, Batch 130, Training Loss: 0.43272748589515686\n",
      "Epoch 2, Batch 140, Training Loss: 0.46652930974960327\n",
      "Epoch 2, Batch 150, Training Loss: 0.3993687629699707\n",
      "Epoch 2, Batch 160, Training Loss: 0.37520816922187805\n",
      "Epoch 2, Batch 170, Training Loss: 0.3935818076133728\n",
      "Epoch 2, Batch 180, Training Loss: 0.40490537881851196\n",
      "Epoch 2, Batch 190, Training Loss: 0.4620359539985657\n",
      "Epoch 2, Batch 200, Training Loss: 0.4817427396774292\n",
      "Epoch 2, Batch 210, Training Loss: 0.3612426221370697\n",
      "Epoch 2, Batch 220, Training Loss: 0.4287869930267334\n",
      "Epoch 2, Batch 230, Training Loss: 0.34147781133651733\n",
      "Epoch 2, Batch 240, Training Loss: 0.408132404088974\n",
      "Epoch 2, Batch 250, Training Loss: 0.3642699122428894\n",
      "Epoch 2, Batch 260, Training Loss: 0.3581404685974121\n",
      "Epoch 2, Average Recall: 0.40196685059324244\n",
      "Epoch 3, Batch 10, Training Loss: 0.37353453040122986\n",
      "Epoch 3, Batch 20, Training Loss: 0.317849338054657\n",
      "Epoch 3, Batch 30, Training Loss: 0.3126561939716339\n",
      "Epoch 3, Batch 40, Training Loss: 0.33455690741539\n",
      "Epoch 3, Batch 50, Training Loss: 0.37050411105155945\n",
      "Epoch 3, Batch 60, Training Loss: 0.32113587856292725\n",
      "Epoch 3, Batch 70, Training Loss: 0.3315730690956116\n",
      "Epoch 3, Batch 80, Training Loss: 0.3309122920036316\n",
      "Epoch 3, Batch 90, Training Loss: 0.3046153485774994\n",
      "Epoch 3, Batch 100, Training Loss: 0.3996996283531189\n",
      "Epoch 3, Batch 110, Training Loss: 0.2962747812271118\n",
      "Epoch 3, Batch 120, Training Loss: 0.35895732045173645\n",
      "Epoch 3, Batch 130, Training Loss: 0.249433696269989\n",
      "Epoch 3, Batch 140, Training Loss: 0.3657644987106323\n",
      "Epoch 3, Batch 150, Training Loss: 0.31526827812194824\n",
      "Epoch 3, Batch 160, Training Loss: 0.33742755651474\n",
      "Epoch 3, Batch 170, Training Loss: 0.3693069517612457\n",
      "Epoch 3, Batch 180, Training Loss: 0.264209121465683\n",
      "Epoch 3, Batch 190, Training Loss: 0.4159277677536011\n",
      "Epoch 3, Batch 200, Training Loss: 0.35689812898635864\n",
      "Epoch 3, Batch 210, Training Loss: 0.2648095488548279\n",
      "Epoch 3, Batch 220, Training Loss: 0.25075170397758484\n",
      "Epoch 3, Batch 230, Training Loss: 0.3460674583911896\n",
      "Epoch 3, Batch 240, Training Loss: 0.31590959429740906\n",
      "Epoch 3, Batch 250, Training Loss: 0.3540442883968353\n",
      "Epoch 3, Batch 260, Training Loss: 0.2190645933151245\n",
      "Epoch 3, Average Recall: 0.40646495665384846\n",
      "Epoch 4, Batch 10, Training Loss: 0.29039159417152405\n",
      "Epoch 4, Batch 20, Training Loss: 0.280140221118927\n",
      "Epoch 4, Batch 30, Training Loss: 0.31108754873275757\n",
      "Epoch 4, Batch 40, Training Loss: 0.34785106778144836\n",
      "Epoch 4, Batch 50, Training Loss: 0.21934282779693604\n",
      "Epoch 4, Batch 60, Training Loss: 0.3003963232040405\n",
      "Epoch 4, Batch 70, Training Loss: 0.3087863028049469\n",
      "Epoch 4, Batch 80, Training Loss: 0.22226132452487946\n",
      "Epoch 4, Batch 90, Training Loss: 0.38759076595306396\n",
      "Epoch 4, Batch 100, Training Loss: 0.34726014733314514\n",
      "Epoch 4, Batch 110, Training Loss: 0.32452502846717834\n",
      "Epoch 4, Batch 120, Training Loss: 0.3881239891052246\n",
      "Epoch 4, Batch 130, Training Loss: 0.22826498746871948\n",
      "Epoch 4, Batch 140, Training Loss: 0.2421332448720932\n",
      "Epoch 4, Batch 150, Training Loss: 0.22622543573379517\n",
      "Epoch 4, Batch 160, Training Loss: 0.32130181789398193\n",
      "Epoch 4, Batch 170, Training Loss: 0.169351726770401\n",
      "Epoch 4, Batch 180, Training Loss: 0.2618880867958069\n",
      "Epoch 4, Batch 190, Training Loss: 0.43056720495224\n",
      "Epoch 4, Batch 200, Training Loss: 0.1969306468963623\n",
      "Epoch 4, Batch 210, Training Loss: 0.29560744762420654\n",
      "Epoch 4, Batch 220, Training Loss: 0.2095578908920288\n",
      "Epoch 4, Batch 230, Training Loss: 0.2701660394668579\n",
      "Epoch 4, Batch 240, Training Loss: 0.2324693500995636\n",
      "Epoch 4, Batch 250, Training Loss: 0.38180163502693176\n",
      "Epoch 4, Batch 260, Training Loss: 0.26652997732162476\n",
      "Epoch 4, Average Recall: 0.4136880971700033\n",
      "Epoch 5, Batch 10, Training Loss: 0.1766403764486313\n",
      "Epoch 5, Batch 20, Training Loss: 0.2792190611362457\n",
      "Epoch 5, Batch 30, Training Loss: 0.22001899778842926\n",
      "Epoch 5, Batch 40, Training Loss: 0.22691945731639862\n",
      "Epoch 5, Batch 50, Training Loss: 0.20951196551322937\n",
      "Epoch 5, Batch 60, Training Loss: 0.2025412619113922\n",
      "Epoch 5, Batch 70, Training Loss: 0.3158843517303467\n",
      "Epoch 5, Batch 80, Training Loss: 0.1623721718788147\n",
      "Epoch 5, Batch 90, Training Loss: 0.16297556459903717\n",
      "Epoch 5, Batch 100, Training Loss: 0.14722853899002075\n",
      "Epoch 5, Batch 110, Training Loss: 0.22382086515426636\n",
      "Epoch 5, Batch 120, Training Loss: 0.25356948375701904\n",
      "Epoch 5, Batch 130, Training Loss: 0.15091340243816376\n",
      "Epoch 5, Batch 140, Training Loss: 0.19867058098316193\n",
      "Epoch 5, Batch 150, Training Loss: 0.19493530690670013\n",
      "Epoch 5, Batch 160, Training Loss: 0.15393182635307312\n",
      "Epoch 5, Batch 170, Training Loss: 0.19326581060886383\n",
      "Epoch 5, Batch 180, Training Loss: 0.17747212946414948\n",
      "Epoch 5, Batch 190, Training Loss: 0.3498705327510834\n",
      "Epoch 5, Batch 200, Training Loss: 0.18278807401657104\n",
      "Epoch 5, Batch 210, Training Loss: 0.2267993688583374\n",
      "Epoch 5, Batch 220, Training Loss: 0.21724896132946014\n",
      "Epoch 5, Batch 230, Training Loss: 0.19316920638084412\n",
      "Epoch 5, Batch 240, Training Loss: 0.222875714302063\n",
      "Epoch 5, Batch 250, Training Loss: 0.23855936527252197\n",
      "Epoch 5, Batch 260, Training Loss: 0.20896771550178528\n",
      "Epoch 5, Average Recall: 0.4149231355620539\n",
      "Epoch 6, Batch 10, Training Loss: 0.15653961896896362\n",
      "Epoch 6, Batch 20, Training Loss: 0.11978796869516373\n",
      "Epoch 6, Batch 30, Training Loss: 0.24954423308372498\n",
      "Epoch 6, Batch 40, Training Loss: 0.1371411681175232\n",
      "Epoch 6, Batch 50, Training Loss: 0.1669931858778\n",
      "Epoch 6, Batch 60, Training Loss: 0.10394851118326187\n",
      "Epoch 6, Batch 70, Training Loss: 0.12465118616819382\n",
      "Epoch 6, Batch 80, Training Loss: 0.1831209808588028\n",
      "Epoch 6, Batch 90, Training Loss: 0.14992021024227142\n",
      "Epoch 6, Batch 100, Training Loss: 0.10684801638126373\n",
      "Epoch 6, Batch 110, Training Loss: 0.10326657444238663\n",
      "Epoch 6, Batch 120, Training Loss: 0.13621757924556732\n",
      "Epoch 6, Batch 130, Training Loss: 0.21300844848155975\n",
      "Epoch 6, Batch 140, Training Loss: 0.15946868062019348\n",
      "Epoch 6, Batch 150, Training Loss: 0.13796581327915192\n",
      "Epoch 6, Batch 160, Training Loss: 0.1186765655875206\n",
      "Epoch 6, Batch 170, Training Loss: 0.19460302591323853\n",
      "Epoch 6, Batch 180, Training Loss: 0.13091619312763214\n",
      "Epoch 6, Batch 190, Training Loss: 0.18382248282432556\n",
      "Epoch 6, Batch 200, Training Loss: 0.17764194309711456\n",
      "Epoch 6, Batch 210, Training Loss: 0.16255857050418854\n",
      "Epoch 6, Batch 220, Training Loss: 0.21268430352210999\n",
      "Epoch 6, Batch 230, Training Loss: 0.20534159243106842\n",
      "Epoch 6, Batch 240, Training Loss: 0.1611708104610443\n",
      "Epoch 6, Batch 250, Training Loss: 0.09397882968187332\n",
      "Epoch 6, Batch 260, Training Loss: 0.1446867287158966\n",
      "Epoch 6, Average Recall: 0.4153972619889243\n",
      "Epoch 7, Batch 10, Training Loss: 0.10835816711187363\n",
      "Epoch 7, Batch 20, Training Loss: 0.11129561066627502\n",
      "Epoch 7, Batch 30, Training Loss: 0.0988750159740448\n",
      "Epoch 7, Batch 40, Training Loss: 0.09070336818695068\n",
      "Epoch 7, Batch 50, Training Loss: 0.10025051236152649\n",
      "Epoch 7, Batch 60, Training Loss: 0.06367336213588715\n",
      "Epoch 7, Batch 70, Training Loss: 0.09991413354873657\n",
      "Epoch 7, Batch 80, Training Loss: 0.14235754311084747\n",
      "Epoch 7, Batch 90, Training Loss: 0.1298033893108368\n",
      "Epoch 7, Batch 100, Training Loss: 0.10124517232179642\n",
      "Epoch 7, Batch 110, Training Loss: 0.08813991397619247\n",
      "Epoch 7, Batch 120, Training Loss: 0.09598783403635025\n",
      "Epoch 7, Batch 130, Training Loss: 0.12769143283367157\n",
      "Epoch 7, Batch 140, Training Loss: 0.1380520462989807\n",
      "Epoch 7, Batch 150, Training Loss: 0.10809314996004105\n",
      "Epoch 7, Batch 160, Training Loss: 0.15644961595535278\n",
      "Epoch 7, Batch 170, Training Loss: 0.09507036954164505\n",
      "Epoch 7, Batch 180, Training Loss: 0.0878838300704956\n",
      "Epoch 7, Batch 190, Training Loss: 0.14156274497509003\n",
      "Epoch 7, Batch 200, Training Loss: 0.07892286032438278\n",
      "Epoch 7, Batch 210, Training Loss: 0.09878437221050262\n",
      "Epoch 7, Batch 220, Training Loss: 0.10582523047924042\n",
      "Epoch 7, Batch 230, Training Loss: 0.08338924497365952\n",
      "Epoch 7, Batch 240, Training Loss: 0.15166790783405304\n",
      "Epoch 7, Batch 250, Training Loss: 0.06911195814609528\n",
      "Epoch 7, Batch 260, Training Loss: 0.08554447442293167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Average Recall: 0.41437194999214483\n",
      "Epoch 8, Batch 10, Training Loss: 0.06792260706424713\n",
      "Epoch 8, Batch 20, Training Loss: 0.060253724455833435\n",
      "Epoch 8, Batch 30, Training Loss: 0.09111098200082779\n",
      "Epoch 8, Batch 40, Training Loss: 0.08811663091182709\n",
      "Epoch 8, Batch 50, Training Loss: 0.08281881362199783\n",
      "Epoch 8, Batch 60, Training Loss: 0.06172093749046326\n",
      "Epoch 8, Batch 70, Training Loss: 0.057946301996707916\n",
      "Epoch 8, Batch 80, Training Loss: 0.09347899258136749\n",
      "Epoch 8, Batch 90, Training Loss: 0.12604793906211853\n",
      "Epoch 8, Batch 100, Training Loss: 0.10107618570327759\n",
      "Epoch 8, Batch 110, Training Loss: 0.054697584360837936\n",
      "Epoch 8, Batch 120, Training Loss: 0.04566571116447449\n",
      "Epoch 8, Batch 130, Training Loss: 0.08005668222904205\n",
      "Epoch 8, Batch 140, Training Loss: 0.03164569288492203\n",
      "Epoch 8, Batch 150, Training Loss: 0.0755399614572525\n",
      "Epoch 8, Batch 160, Training Loss: 0.04697916656732559\n",
      "Epoch 8, Batch 170, Training Loss: 0.04307044669985771\n",
      "Epoch 8, Batch 180, Training Loss: 0.07568302750587463\n",
      "Epoch 8, Batch 190, Training Loss: 0.04206889495253563\n",
      "Epoch 8, Batch 200, Training Loss: 0.06137341633439064\n",
      "Epoch 8, Batch 210, Training Loss: 0.03820295259356499\n",
      "Epoch 8, Batch 220, Training Loss: 0.06510741263628006\n",
      "Epoch 8, Batch 230, Training Loss: 0.06220342963933945\n",
      "Epoch 8, Batch 240, Training Loss: 0.05070408433675766\n",
      "Epoch 8, Batch 250, Training Loss: 0.03837401047348976\n",
      "Epoch 8, Batch 260, Training Loss: 0.05411079525947571\n",
      "Epoch 8, Average Recall: 0.41515987798629633\n",
      "Epoch 9, Batch 10, Training Loss: 0.03871254622936249\n",
      "Epoch 9, Batch 20, Training Loss: 0.033508364111185074\n",
      "Epoch 9, Batch 30, Training Loss: 0.03084949217736721\n",
      "Epoch 9, Batch 40, Training Loss: 0.08043130487203598\n",
      "Epoch 9, Batch 50, Training Loss: 0.03518467769026756\n",
      "Epoch 9, Batch 60, Training Loss: 0.04266231134533882\n",
      "Epoch 9, Batch 70, Training Loss: 0.03911919891834259\n",
      "Epoch 9, Batch 80, Training Loss: 0.03195379674434662\n",
      "Epoch 9, Batch 90, Training Loss: 0.03240508958697319\n",
      "Epoch 9, Batch 100, Training Loss: 0.03735494241118431\n",
      "Epoch 9, Batch 110, Training Loss: 0.03946332633495331\n",
      "Epoch 9, Batch 120, Training Loss: 0.07068172842264175\n",
      "Epoch 9, Batch 130, Training Loss: 0.027057386934757233\n",
      "Epoch 9, Batch 140, Training Loss: 0.02513439953327179\n",
      "Epoch 9, Batch 150, Training Loss: 0.03799222782254219\n",
      "Epoch 9, Batch 160, Training Loss: 0.03180764988064766\n",
      "Epoch 9, Batch 170, Training Loss: 0.03451244533061981\n",
      "Epoch 9, Batch 180, Training Loss: 0.09302122890949249\n",
      "Epoch 9, Batch 190, Training Loss: 0.14010518789291382\n",
      "Epoch 9, Batch 200, Training Loss: 0.02134205587208271\n",
      "Epoch 9, Batch 210, Training Loss: 0.045251794159412384\n",
      "Epoch 9, Batch 220, Training Loss: 0.05663876608014107\n",
      "Epoch 9, Batch 230, Training Loss: 0.028976555913686752\n",
      "Epoch 9, Batch 240, Training Loss: 0.06511463969945908\n",
      "Epoch 9, Batch 250, Training Loss: 0.03710189834237099\n",
      "Epoch 9, Batch 260, Training Loss: 0.04193234443664551\n",
      "Epoch 9, Average Recall: 0.4151079101370724\n",
      "Epoch 10, Batch 10, Training Loss: 0.029683608561754227\n",
      "Epoch 10, Batch 20, Training Loss: 0.020635223016142845\n",
      "Epoch 10, Batch 30, Training Loss: 0.024079635739326477\n",
      "Epoch 10, Batch 40, Training Loss: 0.019873423501849174\n",
      "Epoch 10, Batch 50, Training Loss: 0.024614816531538963\n",
      "Epoch 10, Batch 60, Training Loss: 0.0269846860319376\n",
      "Epoch 10, Batch 70, Training Loss: 0.024641061201691628\n",
      "Epoch 10, Batch 80, Training Loss: 0.0257986132055521\n",
      "Epoch 10, Batch 90, Training Loss: 0.018545307219028473\n",
      "Epoch 10, Batch 100, Training Loss: 0.023621462285518646\n",
      "Epoch 10, Batch 110, Training Loss: 0.019333798438310623\n",
      "Epoch 10, Batch 120, Training Loss: 0.023373402655124664\n",
      "Epoch 10, Batch 130, Training Loss: 0.02949622832238674\n",
      "Epoch 10, Batch 140, Training Loss: 0.024234548211097717\n",
      "Epoch 10, Batch 150, Training Loss: 0.016048001125454903\n",
      "Epoch 10, Batch 160, Training Loss: 0.017782554030418396\n",
      "Epoch 10, Batch 170, Training Loss: 0.028720322996377945\n",
      "Epoch 10, Batch 180, Training Loss: 0.025109194219112396\n",
      "Epoch 10, Batch 190, Training Loss: 0.018556322902441025\n",
      "Epoch 10, Batch 200, Training Loss: 0.04085294529795647\n",
      "Epoch 10, Batch 210, Training Loss: 0.03983606398105621\n",
      "Epoch 10, Batch 220, Training Loss: 0.01707991398870945\n",
      "Epoch 10, Batch 230, Training Loss: 0.014687568880617619\n",
      "Epoch 10, Batch 240, Training Loss: 0.03293323516845703\n",
      "Epoch 10, Batch 250, Training Loss: 0.021800195798277855\n",
      "Epoch 10, Batch 260, Training Loss: 0.0285660233348608\n",
      "Epoch 10, Average Recall: 0.4153446525613148\n",
      "Epoch 1, Batch 10, Training Loss: 0.6717832684516907\n",
      "Epoch 1, Batch 20, Training Loss: 0.5059131383895874\n",
      "Epoch 1, Batch 30, Training Loss: 0.5338472723960876\n",
      "Epoch 1, Batch 40, Training Loss: 0.5101473331451416\n",
      "Epoch 1, Batch 50, Training Loss: 0.45410454273223877\n",
      "Epoch 1, Average Recall: 0.4342190907211068\n",
      "Epoch 2, Batch 10, Training Loss: 0.36108607053756714\n",
      "Epoch 2, Batch 20, Training Loss: 0.459127277135849\n",
      "Epoch 2, Batch 30, Training Loss: 0.3342576026916504\n",
      "Epoch 2, Batch 40, Training Loss: 0.3149573802947998\n",
      "Epoch 2, Batch 50, Training Loss: 0.29169759154319763\n",
      "Epoch 2, Average Recall: 0.45407020734641707\n",
      "Epoch 3, Batch 10, Training Loss: 0.33319464325904846\n",
      "Epoch 3, Batch 20, Training Loss: 0.24111336469650269\n",
      "Epoch 3, Batch 30, Training Loss: 0.2868303656578064\n",
      "Epoch 3, Batch 40, Training Loss: 0.2064794898033142\n",
      "Epoch 3, Batch 50, Training Loss: 0.31819263100624084\n",
      "Epoch 3, Average Recall: 0.4595564046168885\n",
      "Epoch 4, Batch 10, Training Loss: 0.21476052701473236\n",
      "Epoch 4, Batch 20, Training Loss: 0.18036377429962158\n",
      "Epoch 4, Batch 30, Training Loss: 0.19115149974822998\n",
      "Epoch 4, Batch 40, Training Loss: 0.22201880812644958\n",
      "Epoch 4, Batch 50, Training Loss: 0.2429041564464569\n",
      "Epoch 4, Average Recall: 0.4679698661553501\n",
      "Epoch 5, Batch 10, Training Loss: 0.1894521415233612\n",
      "Epoch 5, Batch 20, Training Loss: 0.2377161681652069\n",
      "Epoch 5, Batch 30, Training Loss: 0.20920442044734955\n",
      "Epoch 5, Batch 40, Training Loss: 0.15278849005699158\n",
      "Epoch 5, Batch 50, Training Loss: 0.14810781180858612\n",
      "Epoch 5, Average Recall: 0.4680280237235882\n",
      "Epoch 6, Batch 10, Training Loss: 0.19566898047924042\n",
      "Epoch 6, Batch 20, Training Loss: 0.1503417193889618\n",
      "Epoch 6, Batch 30, Training Loss: 0.21264943480491638\n",
      "Epoch 6, Batch 40, Training Loss: 0.13592861592769623\n",
      "Epoch 6, Batch 50, Training Loss: 0.12872466444969177\n",
      "Epoch 6, Average Recall: 0.4728357160312805\n",
      "Epoch 7, Batch 10, Training Loss: 0.14913304150104523\n",
      "Epoch 7, Batch 20, Training Loss: 0.12050726264715195\n",
      "Epoch 7, Batch 30, Training Loss: 0.12075261771678925\n",
      "Epoch 7, Batch 40, Training Loss: 0.11438335478305817\n",
      "Epoch 7, Batch 50, Training Loss: 0.110771045088768\n",
      "Epoch 7, Average Recall: 0.47044890593277694\n",
      "Epoch 8, Batch 10, Training Loss: 0.12960243225097656\n",
      "Epoch 8, Batch 20, Training Loss: 0.1538984626531601\n",
      "Epoch 8, Batch 30, Training Loss: 0.08166801929473877\n",
      "Epoch 8, Batch 40, Training Loss: 0.1323423534631729\n",
      "Epoch 8, Batch 50, Training Loss: 0.1615496724843979\n",
      "Epoch 8, Average Recall: 0.4734343277690052\n",
      "Epoch 9, Batch 10, Training Loss: 0.08895862847566605\n",
      "Epoch 9, Batch 20, Training Loss: 0.051801785826683044\n",
      "Epoch 9, Batch 30, Training Loss: 0.06764177978038788\n",
      "Epoch 9, Batch 40, Training Loss: 0.06424516439437866\n",
      "Epoch 9, Batch 50, Training Loss: 0.06739117205142975\n",
      "Epoch 9, Average Recall: 0.47279459451838485\n",
      "Epoch 10, Batch 10, Training Loss: 0.05022259056568146\n",
      "Epoch 10, Batch 20, Training Loss: 0.04993009194731712\n",
      "Epoch 10, Batch 30, Training Loss: 0.055466435849666595\n",
      "Epoch 10, Batch 40, Training Loss: 0.028192035853862762\n",
      "Epoch 10, Batch 50, Training Loss: 0.036194492131471634\n",
      "Epoch 10, Average Recall: 0.47103048161515904\n",
      "Epoch 1, Batch 10, Training Loss: 0.7293597459793091\n",
      "Epoch 1, Batch 20, Training Loss: 0.7044745087623596\n",
      "Epoch 1, Average Recall: 0.40538133356790995\n",
      "Epoch 2, Batch 10, Training Loss: 0.443586528301239\n",
      "Epoch 2, Batch 20, Training Loss: 0.37125325202941895\n",
      "Epoch 2, Average Recall: 0.47681980119634065\n",
      "Epoch 3, Batch 10, Training Loss: 0.3428027629852295\n",
      "Epoch 3, Batch 20, Training Loss: 0.30201929807662964\n",
      "Epoch 3, Average Recall: 0.5020705049261084\n",
      "Epoch 4, Batch 10, Training Loss: 0.29394084215164185\n",
      "Epoch 4, Batch 20, Training Loss: 0.2552536129951477\n",
      "Epoch 4, Average Recall: 0.5101535010555946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Batch 10, Training Loss: 0.17719459533691406\n",
      "Epoch 5, Batch 20, Training Loss: 0.1680706888437271\n",
      "Epoch 5, Average Recall: 0.5174678923293455\n",
      "Epoch 6, Batch 10, Training Loss: 0.10044483095407486\n",
      "Epoch 6, Batch 20, Training Loss: 0.1479000449180603\n",
      "Epoch 6, Average Recall: 0.5220476337086558\n",
      "Epoch 7, Batch 10, Training Loss: 0.08831409364938736\n",
      "Epoch 7, Batch 20, Training Loss: 0.09064898639917374\n",
      "Epoch 7, Average Recall: 0.5272453377902885\n",
      "Epoch 8, Batch 10, Training Loss: 0.122334785759449\n",
      "Epoch 8, Batch 20, Training Loss: 0.06328240036964417\n",
      "Epoch 8, Average Recall: 0.5261292663617171\n",
      "Epoch 9, Batch 10, Training Loss: 0.053841400891542435\n",
      "Epoch 9, Batch 20, Training Loss: 0.052592769265174866\n",
      "Epoch 9, Average Recall: 0.5283614092188599\n",
      "Epoch 10, Batch 10, Training Loss: 0.07987112551927567\n",
      "Epoch 10, Batch 20, Training Loss: 0.04171677678823471\n",
      "Epoch 10, Average Recall: 0.5273607934553131\n",
      "Epoch 1, Batch 10, Training Loss: 0.6639120578765869\n",
      "Epoch 1, Batch 20, Training Loss: 0.5906025171279907\n",
      "Epoch 1, Batch 30, Training Loss: 0.5728281736373901\n",
      "Epoch 1, Average Recall: 0.42418075817860296\n",
      "Epoch 2, Batch 10, Training Loss: 0.4419570565223694\n",
      "Epoch 2, Batch 20, Training Loss: 0.47349846363067627\n",
      "Epoch 2, Batch 30, Training Loss: 0.5405051708221436\n",
      "Epoch 2, Average Recall: 0.4479491324049514\n",
      "Epoch 3, Batch 10, Training Loss: 0.41003546118736267\n",
      "Epoch 3, Batch 20, Training Loss: 0.3908301591873169\n",
      "Epoch 3, Batch 30, Training Loss: 0.4109548330307007\n",
      "Epoch 3, Average Recall: 0.45894880083996464\n",
      "Epoch 4, Batch 10, Training Loss: 0.20984810590744019\n",
      "Epoch 4, Batch 20, Training Loss: 0.30151909589767456\n",
      "Epoch 4, Batch 30, Training Loss: 0.20541071891784668\n",
      "Epoch 4, Average Recall: 0.46137336980548194\n",
      "Epoch 5, Batch 10, Training Loss: 0.23208273947238922\n",
      "Epoch 5, Batch 20, Training Loss: 0.20679134130477905\n",
      "Epoch 5, Batch 30, Training Loss: 0.26300549507141113\n",
      "Epoch 5, Average Recall: 0.46233490826702034\n",
      "Epoch 6, Batch 10, Training Loss: 0.21654406189918518\n",
      "Epoch 6, Batch 20, Training Loss: 0.20508019626140594\n",
      "Epoch 6, Batch 30, Training Loss: 0.18986724317073822\n",
      "Epoch 6, Average Recall: 0.4694469772325375\n",
      "Epoch 7, Batch 10, Training Loss: 0.145783931016922\n",
      "Epoch 7, Batch 20, Training Loss: 0.11298239231109619\n",
      "Epoch 7, Batch 30, Training Loss: 0.1546509563922882\n",
      "Epoch 7, Average Recall: 0.4678844772325375\n",
      "Epoch 8, Batch 10, Training Loss: 0.13714540004730225\n",
      "Epoch 8, Batch 20, Training Loss: 0.12065517902374268\n",
      "Epoch 8, Batch 30, Training Loss: 0.10783116519451141\n",
      "Epoch 8, Average Recall: 0.4669229387709991\n",
      "Epoch 9, Batch 10, Training Loss: 0.12348679453134537\n",
      "Epoch 9, Batch 20, Training Loss: 0.07066816836595535\n",
      "Epoch 9, Batch 30, Training Loss: 0.12220720946788788\n",
      "Epoch 9, Average Recall: 0.46508068081343945\n",
      "Epoch 10, Batch 10, Training Loss: 0.10605752468109131\n",
      "Epoch 10, Batch 20, Training Loss: 0.07886246591806412\n",
      "Epoch 10, Batch 30, Training Loss: 0.09388574212789536\n",
      "Epoch 10, Average Recall: 0.4656816423519009\n",
      "Epoch 1, Batch 10, Training Loss: 0.6794314980506897\n",
      "Epoch 1, Batch 20, Training Loss: 0.64264976978302\n",
      "Epoch 1, Batch 30, Training Loss: 0.6385900974273682\n",
      "Epoch 1, Batch 40, Training Loss: 0.6103707551956177\n",
      "Epoch 1, Batch 50, Training Loss: 0.5949335694313049\n",
      "Epoch 1, Batch 60, Training Loss: 0.5724972486495972\n",
      "Epoch 1, Batch 70, Training Loss: 0.49411478638648987\n",
      "Epoch 1, Batch 80, Training Loss: 0.5358486771583557\n",
      "Epoch 1, Batch 90, Training Loss: 0.4645672142505646\n",
      "Epoch 1, Batch 100, Training Loss: 0.6013351678848267\n",
      "Epoch 1, Batch 110, Training Loss: 0.5170354247093201\n",
      "Epoch 1, Batch 120, Training Loss: 0.5898755192756653\n",
      "Epoch 1, Batch 130, Training Loss: 0.49005642533302307\n",
      "Epoch 1, Batch 140, Training Loss: 0.511597752571106\n",
      "Epoch 1, Batch 150, Training Loss: 0.6112005710601807\n",
      "Epoch 1, Batch 160, Training Loss: 0.4594932496547699\n",
      "Epoch 1, Batch 170, Training Loss: 0.4677314758300781\n",
      "Epoch 1, Batch 180, Training Loss: 0.47461315989494324\n",
      "Epoch 1, Batch 190, Training Loss: 0.5295968651771545\n",
      "Epoch 1, Batch 200, Training Loss: 0.5317120552062988\n",
      "Epoch 1, Batch 210, Training Loss: 0.4748521149158478\n",
      "Epoch 1, Batch 220, Training Loss: 0.5941796898841858\n",
      "Epoch 1, Batch 230, Training Loss: 0.48638391494750977\n",
      "Epoch 1, Batch 240, Training Loss: 0.35806405544281006\n",
      "Epoch 1, Batch 250, Training Loss: 0.5000854134559631\n",
      "Epoch 1, Batch 260, Training Loss: 0.45343098044395447\n",
      "Epoch 1, Batch 270, Training Loss: 0.3532370328903198\n",
      "Epoch 1, Batch 280, Training Loss: 0.5320543050765991\n",
      "Epoch 1, Batch 290, Training Loss: 0.31433627009391785\n",
      "Epoch 1, Batch 300, Training Loss: 0.4652974009513855\n",
      "Epoch 1, Batch 310, Training Loss: 0.4179730713367462\n",
      "Epoch 1, Batch 320, Training Loss: 0.3961130380630493\n",
      "Epoch 1, Batch 330, Training Loss: 0.3620913624763489\n",
      "Epoch 1, Batch 340, Training Loss: 0.3654975891113281\n",
      "Epoch 1, Batch 350, Training Loss: 0.44616934657096863\n",
      "Epoch 1, Batch 360, Training Loss: 0.508942186832428\n",
      "Epoch 1, Batch 370, Training Loss: 0.28463149070739746\n",
      "Epoch 1, Batch 380, Training Loss: 0.3563218116760254\n",
      "Epoch 1, Batch 390, Training Loss: 0.489101767539978\n",
      "Epoch 1, Batch 400, Training Loss: 0.43106400966644287\n",
      "Epoch 1, Batch 410, Training Loss: 0.38366395235061646\n",
      "Epoch 1, Batch 420, Training Loss: 0.47608378529548645\n",
      "Epoch 1, Batch 430, Training Loss: 0.49582719802856445\n",
      "Epoch 1, Batch 440, Training Loss: 0.47871720790863037\n",
      "Epoch 1, Batch 450, Training Loss: 0.45684048533439636\n",
      "Epoch 1, Batch 460, Training Loss: 0.30891212821006775\n",
      "Epoch 1, Batch 470, Training Loss: 0.5224024653434753\n",
      "Epoch 1, Batch 480, Training Loss: 0.3311467170715332\n",
      "Epoch 1, Batch 490, Training Loss: 0.4393629729747772\n",
      "Epoch 1, Batch 500, Training Loss: 0.4997822940349579\n",
      "Epoch 1, Average Recall: 0.41469937796949236\n",
      "Epoch 2, Batch 10, Training Loss: 0.44161540269851685\n",
      "Epoch 2, Batch 20, Training Loss: 0.3909417390823364\n",
      "Epoch 2, Batch 30, Training Loss: 0.41790974140167236\n",
      "Epoch 2, Batch 40, Training Loss: 0.3093690276145935\n",
      "Epoch 2, Batch 50, Training Loss: 0.3758573532104492\n",
      "Epoch 2, Batch 60, Training Loss: 0.30522632598876953\n",
      "Epoch 2, Batch 70, Training Loss: 0.4516597092151642\n",
      "Epoch 2, Batch 80, Training Loss: 0.42201703786849976\n",
      "Epoch 2, Batch 90, Training Loss: 0.4594375789165497\n",
      "Epoch 2, Batch 100, Training Loss: 0.4201294183731079\n",
      "Epoch 2, Batch 110, Training Loss: 0.3013940751552582\n",
      "Epoch 2, Batch 120, Training Loss: 0.30352991819381714\n",
      "Epoch 2, Batch 130, Training Loss: 0.3542284369468689\n",
      "Epoch 2, Batch 140, Training Loss: 0.32799428701400757\n",
      "Epoch 2, Batch 150, Training Loss: 0.4585418999195099\n",
      "Epoch 2, Batch 160, Training Loss: 0.49455463886260986\n",
      "Epoch 2, Batch 170, Training Loss: 0.33284395933151245\n",
      "Epoch 2, Batch 180, Training Loss: 0.3411637842655182\n",
      "Epoch 2, Batch 190, Training Loss: 0.40495049953460693\n",
      "Epoch 2, Batch 200, Training Loss: 0.2543379068374634\n",
      "Epoch 2, Batch 210, Training Loss: 0.3563000559806824\n",
      "Epoch 2, Batch 220, Training Loss: 0.3373510241508484\n",
      "Epoch 2, Batch 230, Training Loss: 0.38050228357315063\n",
      "Epoch 2, Batch 240, Training Loss: 0.297465056180954\n",
      "Epoch 2, Batch 250, Training Loss: 0.3271109461784363\n",
      "Epoch 2, Batch 260, Training Loss: 0.2666361629962921\n",
      "Epoch 2, Batch 270, Training Loss: 0.3802775740623474\n",
      "Epoch 2, Batch 280, Training Loss: 0.47925013303756714\n",
      "Epoch 2, Batch 290, Training Loss: 0.3590717017650604\n",
      "Epoch 2, Batch 300, Training Loss: 0.4069679379463196\n",
      "Epoch 2, Batch 310, Training Loss: 0.2494840919971466\n",
      "Epoch 2, Batch 320, Training Loss: 0.30814024806022644\n",
      "Epoch 2, Batch 330, Training Loss: 0.3083496689796448\n",
      "Epoch 2, Batch 340, Training Loss: 0.21001794934272766\n",
      "Epoch 2, Batch 350, Training Loss: 0.28150007128715515\n",
      "Epoch 2, Batch 360, Training Loss: 0.49458742141723633\n",
      "Epoch 2, Batch 370, Training Loss: 0.3426472544670105\n",
      "Epoch 2, Batch 380, Training Loss: 0.29797056317329407\n",
      "Epoch 2, Batch 390, Training Loss: 0.4974098801612854\n",
      "Epoch 2, Batch 400, Training Loss: 0.37505829334259033\n",
      "Epoch 2, Batch 410, Training Loss: 0.3270603120326996\n",
      "Epoch 2, Batch 420, Training Loss: 0.40164268016815186\n",
      "Epoch 2, Batch 430, Training Loss: 0.3129239082336426\n",
      "Epoch 2, Batch 440, Training Loss: 0.45854833722114563\n",
      "Epoch 2, Batch 450, Training Loss: 0.2910553514957428\n",
      "Epoch 2, Batch 460, Training Loss: 0.2161712944507599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 470, Training Loss: 0.41293787956237793\n",
      "Epoch 2, Batch 480, Training Loss: 0.234572172164917\n",
      "Epoch 2, Batch 490, Training Loss: 0.42211195826530457\n",
      "Epoch 2, Batch 500, Training Loss: 0.31346461176872253\n",
      "Epoch 2, Average Recall: 0.42579394848712177\n",
      "Epoch 3, Batch 10, Training Loss: 0.25363510847091675\n",
      "Epoch 3, Batch 20, Training Loss: 0.23124322295188904\n",
      "Epoch 3, Batch 30, Training Loss: 0.21626052260398865\n",
      "Epoch 3, Batch 40, Training Loss: 0.24553976953029633\n",
      "Epoch 3, Batch 50, Training Loss: 0.20845451951026917\n",
      "Epoch 3, Batch 60, Training Loss: 0.39466428756713867\n",
      "Epoch 3, Batch 70, Training Loss: 0.3123512864112854\n",
      "Epoch 3, Batch 80, Training Loss: 0.1938730627298355\n",
      "Epoch 3, Batch 90, Training Loss: 0.2716210186481476\n",
      "Epoch 3, Batch 100, Training Loss: 0.2948238253593445\n",
      "Epoch 3, Batch 110, Training Loss: 0.28519406914711\n",
      "Epoch 3, Batch 120, Training Loss: 0.26350945234298706\n",
      "Epoch 3, Batch 130, Training Loss: 0.2423400729894638\n",
      "Epoch 3, Batch 140, Training Loss: 0.26554471254348755\n",
      "Epoch 3, Batch 150, Training Loss: 0.2279203087091446\n",
      "Epoch 3, Batch 160, Training Loss: 0.3348855674266815\n",
      "Epoch 3, Batch 170, Training Loss: 0.20074892044067383\n",
      "Epoch 3, Batch 180, Training Loss: 0.2063034176826477\n",
      "Epoch 3, Batch 190, Training Loss: 0.20735232532024384\n",
      "Epoch 3, Batch 200, Training Loss: 0.2456619292497635\n",
      "Epoch 3, Batch 210, Training Loss: 0.2257414013147354\n",
      "Epoch 3, Batch 220, Training Loss: 0.21945852041244507\n",
      "Epoch 3, Batch 230, Training Loss: 0.22712931036949158\n",
      "Epoch 3, Batch 240, Training Loss: 0.2389896959066391\n",
      "Epoch 3, Batch 250, Training Loss: 0.25476619601249695\n",
      "Epoch 3, Batch 260, Training Loss: 0.23412099480628967\n",
      "Epoch 3, Batch 270, Training Loss: 0.24853040277957916\n",
      "Epoch 3, Batch 280, Training Loss: 0.29948437213897705\n",
      "Epoch 3, Batch 290, Training Loss: 0.24906301498413086\n",
      "Epoch 3, Batch 300, Training Loss: 0.19396471977233887\n",
      "Epoch 3, Batch 310, Training Loss: 0.2701846659183502\n",
      "Epoch 3, Batch 320, Training Loss: 0.2485540509223938\n",
      "Epoch 3, Batch 330, Training Loss: 0.21969421207904816\n",
      "Epoch 3, Batch 340, Training Loss: 0.2934556007385254\n",
      "Epoch 3, Batch 350, Training Loss: 0.31488391757011414\n",
      "Epoch 3, Batch 360, Training Loss: 0.21519669890403748\n",
      "Epoch 3, Batch 370, Training Loss: 0.1958913505077362\n",
      "Epoch 3, Batch 380, Training Loss: 0.2303399294614792\n",
      "Epoch 3, Batch 390, Training Loss: 0.27152958512306213\n",
      "Epoch 3, Batch 400, Training Loss: 0.2386627197265625\n",
      "Epoch 3, Batch 410, Training Loss: 0.15732495486736298\n",
      "Epoch 3, Batch 420, Training Loss: 0.2400929033756256\n",
      "Epoch 3, Batch 430, Training Loss: 0.11739285290241241\n",
      "Epoch 3, Batch 440, Training Loss: 0.2903655171394348\n",
      "Epoch 3, Batch 450, Training Loss: 0.33886995911598206\n",
      "Epoch 3, Batch 460, Training Loss: 0.2886667549610138\n",
      "Epoch 3, Batch 470, Training Loss: 0.3212530314922333\n",
      "Epoch 3, Batch 480, Training Loss: 0.2511061429977417\n",
      "Epoch 3, Batch 490, Training Loss: 0.24845397472381592\n",
      "Epoch 3, Batch 500, Training Loss: 0.28859850764274597\n",
      "Epoch 3, Average Recall: 0.4303400068767192\n",
      "Epoch 4, Batch 10, Training Loss: 0.1902008056640625\n",
      "Epoch 4, Batch 20, Training Loss: 0.2659543752670288\n",
      "Epoch 4, Batch 30, Training Loss: 0.18608145415782928\n",
      "Epoch 4, Batch 40, Training Loss: 0.20336486399173737\n",
      "Epoch 4, Batch 50, Training Loss: 0.22959142923355103\n",
      "Epoch 4, Batch 60, Training Loss: 0.14946812391281128\n",
      "Epoch 4, Batch 70, Training Loss: 0.1393251121044159\n",
      "Epoch 4, Batch 80, Training Loss: 0.14329704642295837\n",
      "Epoch 4, Batch 90, Training Loss: 0.2414591908454895\n",
      "Epoch 4, Batch 100, Training Loss: 0.15894724428653717\n",
      "Epoch 4, Batch 110, Training Loss: 0.24407793581485748\n",
      "Epoch 4, Batch 120, Training Loss: 0.17621739208698273\n",
      "Epoch 4, Batch 130, Training Loss: 0.13312824070453644\n",
      "Epoch 4, Batch 140, Training Loss: 0.3034948706626892\n",
      "Epoch 4, Batch 150, Training Loss: 0.22359758615493774\n",
      "Epoch 4, Batch 160, Training Loss: 0.2226635366678238\n",
      "Epoch 4, Batch 170, Training Loss: 0.2512862980365753\n",
      "Epoch 4, Batch 180, Training Loss: 0.1573466956615448\n",
      "Epoch 4, Batch 190, Training Loss: 0.2571912705898285\n",
      "Epoch 4, Batch 200, Training Loss: 0.1981005072593689\n",
      "Epoch 4, Batch 210, Training Loss: 0.14133447408676147\n",
      "Epoch 4, Batch 220, Training Loss: 0.2085813730955124\n",
      "Epoch 4, Batch 230, Training Loss: 0.24823790788650513\n",
      "Epoch 4, Batch 240, Training Loss: 0.19571083784103394\n",
      "Epoch 4, Batch 250, Training Loss: 0.13572648167610168\n",
      "Epoch 4, Batch 260, Training Loss: 0.18290121853351593\n",
      "Epoch 4, Batch 270, Training Loss: 0.1618805080652237\n",
      "Epoch 4, Batch 280, Training Loss: 0.2569485902786255\n",
      "Epoch 4, Batch 290, Training Loss: 0.1874004602432251\n",
      "Epoch 4, Batch 300, Training Loss: 0.12182942032814026\n",
      "Epoch 4, Batch 310, Training Loss: 0.18376006186008453\n",
      "Epoch 4, Batch 320, Training Loss: 0.1324291229248047\n",
      "Epoch 4, Batch 330, Training Loss: 0.20244885981082916\n",
      "Epoch 4, Batch 340, Training Loss: 0.17518013715744019\n",
      "Epoch 4, Batch 350, Training Loss: 0.1555159091949463\n",
      "Epoch 4, Batch 360, Training Loss: 0.16896720230579376\n",
      "Epoch 4, Batch 370, Training Loss: 0.1279684156179428\n",
      "Epoch 4, Batch 380, Training Loss: 0.28961485624313354\n",
      "Epoch 4, Batch 390, Training Loss: 0.2155357003211975\n",
      "Epoch 4, Batch 400, Training Loss: 0.12915225327014923\n",
      "Epoch 4, Batch 410, Training Loss: 0.25150948762893677\n",
      "Epoch 4, Batch 420, Training Loss: 0.1771942526102066\n",
      "Epoch 4, Batch 430, Training Loss: 0.16707371175289154\n",
      "Epoch 4, Batch 440, Training Loss: 0.19210828840732574\n",
      "Epoch 4, Batch 450, Training Loss: 0.2295653522014618\n",
      "Epoch 4, Batch 460, Training Loss: 0.20165207982063293\n",
      "Epoch 4, Batch 470, Training Loss: 0.17738425731658936\n",
      "Epoch 4, Batch 480, Training Loss: 0.26896387338638306\n",
      "Epoch 4, Batch 490, Training Loss: 0.20154470205307007\n",
      "Epoch 4, Batch 500, Training Loss: 0.14002229273319244\n",
      "Epoch 4, Average Recall: 0.4316743248312078\n",
      "Epoch 5, Batch 10, Training Loss: 0.29055529832839966\n",
      "Epoch 5, Batch 20, Training Loss: 0.09391706436872482\n",
      "Epoch 5, Batch 30, Training Loss: 0.15855127573013306\n",
      "Epoch 5, Batch 40, Training Loss: 0.12418404221534729\n",
      "Epoch 5, Batch 50, Training Loss: 0.1162385642528534\n",
      "Epoch 5, Batch 60, Training Loss: 0.07621680200099945\n",
      "Epoch 5, Batch 70, Training Loss: 0.12799303233623505\n",
      "Epoch 5, Batch 80, Training Loss: 0.14435510337352753\n",
      "Epoch 5, Batch 90, Training Loss: 0.05967685580253601\n",
      "Epoch 5, Batch 100, Training Loss: 0.10498858243227005\n",
      "Epoch 5, Batch 110, Training Loss: 0.22766783833503723\n",
      "Epoch 5, Batch 120, Training Loss: 0.11948231607675552\n",
      "Epoch 5, Batch 130, Training Loss: 0.10041267424821854\n",
      "Epoch 5, Batch 140, Training Loss: 0.14581158757209778\n",
      "Epoch 5, Batch 150, Training Loss: 0.17704804241657257\n",
      "Epoch 5, Batch 160, Training Loss: 0.1272016167640686\n",
      "Epoch 5, Batch 170, Training Loss: 0.07453630864620209\n",
      "Epoch 5, Batch 180, Training Loss: 0.0973839983344078\n",
      "Epoch 5, Batch 190, Training Loss: 0.09965664893388748\n",
      "Epoch 5, Batch 200, Training Loss: 0.13530030846595764\n",
      "Epoch 5, Batch 210, Training Loss: 0.16251683235168457\n",
      "Epoch 5, Batch 220, Training Loss: 0.18849939107894897\n",
      "Epoch 5, Batch 230, Training Loss: 0.1598336100578308\n",
      "Epoch 5, Batch 240, Training Loss: 0.09466229379177094\n",
      "Epoch 5, Batch 250, Training Loss: 0.13337433338165283\n",
      "Epoch 5, Batch 260, Training Loss: 0.11973237246274948\n",
      "Epoch 5, Batch 270, Training Loss: 0.13779477775096893\n",
      "Epoch 5, Batch 280, Training Loss: 0.09693825244903564\n",
      "Epoch 5, Batch 290, Training Loss: 0.12388460338115692\n",
      "Epoch 5, Batch 300, Training Loss: 0.1134234368801117\n",
      "Epoch 5, Batch 310, Training Loss: 0.06586366146802902\n",
      "Epoch 5, Batch 320, Training Loss: 0.15204739570617676\n",
      "Epoch 5, Batch 330, Training Loss: 0.09480821341276169\n",
      "Epoch 5, Batch 340, Training Loss: 0.17579826712608337\n",
      "Epoch 5, Batch 350, Training Loss: 0.07279373705387115\n",
      "Epoch 5, Batch 360, Training Loss: 0.14639709889888763\n",
      "Epoch 5, Batch 370, Training Loss: 0.18087340891361237\n",
      "Epoch 5, Batch 380, Training Loss: 0.08428733050823212\n",
      "Epoch 5, Batch 390, Training Loss: 0.13879568874835968\n",
      "Epoch 5, Batch 400, Training Loss: 0.16881133615970612\n",
      "Epoch 5, Batch 410, Training Loss: 0.138723224401474\n",
      "Epoch 5, Batch 420, Training Loss: 0.15098467469215393\n",
      "Epoch 5, Batch 430, Training Loss: 0.14476677775382996\n",
      "Epoch 5, Batch 440, Training Loss: 0.0760878399014473\n",
      "Epoch 5, Batch 450, Training Loss: 0.2327004373073578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Batch 460, Training Loss: 0.11313357204198837\n",
      "Epoch 5, Batch 470, Training Loss: 0.11629787087440491\n",
      "Epoch 5, Batch 480, Training Loss: 0.09969624131917953\n",
      "Epoch 5, Batch 490, Training Loss: 0.10199227929115295\n",
      "Epoch 5, Batch 500, Training Loss: 0.1620100736618042\n",
      "Epoch 5, Average Recall: 0.43263940985246313\n",
      "Epoch 6, Batch 10, Training Loss: 0.05287185311317444\n",
      "Epoch 6, Batch 20, Training Loss: 0.06785270571708679\n",
      "Epoch 6, Batch 30, Training Loss: 0.15322494506835938\n",
      "Epoch 6, Batch 40, Training Loss: 0.12365461885929108\n",
      "Epoch 6, Batch 50, Training Loss: 0.047182731330394745\n",
      "Epoch 6, Batch 60, Training Loss: 0.059053897857666016\n",
      "Epoch 6, Batch 70, Training Loss: 0.034851688891649246\n",
      "Epoch 6, Batch 80, Training Loss: 0.11010454595088959\n",
      "Epoch 6, Batch 90, Training Loss: 0.05460668355226517\n",
      "Epoch 6, Batch 100, Training Loss: 0.12236916273832321\n",
      "Epoch 6, Batch 110, Training Loss: 0.0497581921517849\n",
      "Epoch 6, Batch 120, Training Loss: 0.09251844137907028\n",
      "Epoch 6, Batch 130, Training Loss: 0.05517103150486946\n",
      "Epoch 6, Batch 140, Training Loss: 0.059242065995931625\n",
      "Epoch 6, Batch 150, Training Loss: 0.06649734824895859\n",
      "Epoch 6, Batch 160, Training Loss: 0.05924316868185997\n",
      "Epoch 6, Batch 170, Training Loss: 0.0678655132651329\n",
      "Epoch 6, Batch 180, Training Loss: 0.09826282411813736\n",
      "Epoch 6, Batch 190, Training Loss: 0.06558394432067871\n",
      "Epoch 6, Batch 200, Training Loss: 0.05644460394978523\n",
      "Epoch 6, Batch 210, Training Loss: 0.0764840692281723\n",
      "Epoch 6, Batch 220, Training Loss: 0.05333990976214409\n",
      "Epoch 6, Batch 230, Training Loss: 0.10242046415805817\n",
      "Epoch 6, Batch 240, Training Loss: 0.06473430246114731\n",
      "Epoch 6, Batch 250, Training Loss: 0.04647841677069664\n",
      "Epoch 6, Batch 260, Training Loss: 0.06363900750875473\n",
      "Epoch 6, Batch 270, Training Loss: 0.09138986468315125\n",
      "Epoch 6, Batch 280, Training Loss: 0.0556025356054306\n",
      "Epoch 6, Batch 290, Training Loss: 0.12206396460533142\n",
      "Epoch 6, Batch 300, Training Loss: 0.05214853584766388\n",
      "Epoch 6, Batch 310, Training Loss: 0.06421161442995071\n",
      "Epoch 6, Batch 320, Training Loss: 0.037119634449481964\n",
      "Epoch 6, Batch 330, Training Loss: 0.0773642435669899\n",
      "Epoch 6, Batch 340, Training Loss: 0.09354183822870255\n",
      "Epoch 6, Batch 350, Training Loss: 0.12868821620941162\n",
      "Epoch 6, Batch 360, Training Loss: 0.03272320330142975\n",
      "Epoch 6, Batch 370, Training Loss: 0.07479489594697952\n",
      "Epoch 6, Batch 380, Training Loss: 0.05960673838853836\n",
      "Epoch 6, Batch 390, Training Loss: 0.11238819360733032\n",
      "Epoch 6, Batch 400, Training Loss: 0.07336042821407318\n",
      "Epoch 6, Batch 410, Training Loss: 0.05563431233167648\n",
      "Epoch 6, Batch 420, Training Loss: 0.11182135343551636\n",
      "Epoch 6, Batch 430, Training Loss: 0.05768201872706413\n",
      "Epoch 6, Batch 440, Training Loss: 0.08812042325735092\n",
      "Epoch 6, Batch 450, Training Loss: 0.08239832520484924\n",
      "Epoch 6, Batch 460, Training Loss: 0.11432278156280518\n",
      "Epoch 6, Batch 470, Training Loss: 0.05544358491897583\n",
      "Epoch 6, Batch 480, Training Loss: 0.05516395717859268\n",
      "Epoch 6, Batch 490, Training Loss: 0.09238878637552261\n",
      "Epoch 6, Batch 500, Training Loss: 0.14419756829738617\n",
      "Epoch 6, Average Recall: 0.43276053388347085\n",
      "Epoch 7, Batch 10, Training Loss: 0.04105890169739723\n",
      "Epoch 7, Batch 20, Training Loss: 0.038329314440488815\n",
      "Epoch 7, Batch 30, Training Loss: 0.029782284051179886\n",
      "Epoch 7, Batch 40, Training Loss: 0.034331709146499634\n",
      "Epoch 7, Batch 50, Training Loss: 0.04391330108046532\n",
      "Epoch 7, Batch 60, Training Loss: 0.03441465646028519\n",
      "Epoch 7, Batch 70, Training Loss: 0.046879809349775314\n",
      "Epoch 7, Batch 80, Training Loss: 0.08063111454248428\n",
      "Epoch 7, Batch 90, Training Loss: 0.05281145125627518\n",
      "Epoch 7, Batch 100, Training Loss: 0.05779816955327988\n",
      "Epoch 7, Batch 110, Training Loss: 0.048464320600032806\n",
      "Epoch 7, Batch 120, Training Loss: 0.04053137078881264\n",
      "Epoch 7, Batch 130, Training Loss: 0.08746840059757233\n",
      "Epoch 7, Batch 140, Training Loss: 0.04480263963341713\n",
      "Epoch 7, Batch 150, Training Loss: 0.04239526018500328\n",
      "Epoch 7, Batch 160, Training Loss: 0.027450060471892357\n",
      "Epoch 7, Batch 170, Training Loss: 0.02164272405207157\n",
      "Epoch 7, Batch 180, Training Loss: 0.023190349340438843\n",
      "Epoch 7, Batch 190, Training Loss: 0.04570931941270828\n",
      "Epoch 7, Batch 200, Training Loss: 0.04400058463215828\n",
      "Epoch 7, Batch 210, Training Loss: 0.017829935997724533\n",
      "Epoch 7, Batch 220, Training Loss: 0.03147479519248009\n",
      "Epoch 7, Batch 230, Training Loss: 0.034003231674432755\n",
      "Epoch 7, Batch 240, Training Loss: 0.051708824932575226\n",
      "Epoch 7, Batch 250, Training Loss: 0.07587637007236481\n",
      "Epoch 7, Batch 260, Training Loss: 0.05705726146697998\n",
      "Epoch 7, Batch 270, Training Loss: 0.04429430514574051\n",
      "Epoch 7, Batch 280, Training Loss: 0.04727088660001755\n",
      "Epoch 7, Batch 290, Training Loss: 0.06737801432609558\n",
      "Epoch 7, Batch 300, Training Loss: 0.023074530065059662\n",
      "Epoch 7, Batch 310, Training Loss: 0.03739846497774124\n",
      "Epoch 7, Batch 320, Training Loss: 0.04144361987709999\n",
      "Epoch 7, Batch 330, Training Loss: 0.06736855208873749\n",
      "Epoch 7, Batch 340, Training Loss: 0.022455384954810143\n",
      "Epoch 7, Batch 350, Training Loss: 0.032624877989292145\n",
      "Epoch 7, Batch 360, Training Loss: 0.030865183100104332\n",
      "Epoch 7, Batch 370, Training Loss: 0.0310857892036438\n",
      "Epoch 7, Batch 380, Training Loss: 0.043143995106220245\n",
      "Epoch 7, Batch 390, Training Loss: 0.05333208292722702\n",
      "Epoch 7, Batch 400, Training Loss: 0.03557896614074707\n",
      "Epoch 7, Batch 410, Training Loss: 0.019060898572206497\n",
      "Epoch 7, Batch 420, Training Loss: 0.03665822744369507\n",
      "Epoch 7, Batch 430, Training Loss: 0.042338572442531586\n",
      "Epoch 7, Batch 440, Training Loss: 0.03642397001385689\n",
      "Epoch 7, Batch 450, Training Loss: 0.055942460894584656\n",
      "Epoch 7, Batch 460, Training Loss: 0.06518620997667313\n",
      "Epoch 7, Batch 470, Training Loss: 0.033155035227537155\n",
      "Epoch 7, Batch 480, Training Loss: 0.06948504596948624\n",
      "Epoch 7, Batch 490, Training Loss: 0.04283181577920914\n",
      "Epoch 7, Batch 500, Training Loss: 0.038019999861717224\n",
      "Epoch 7, Average Recall: 0.4319673668417104\n",
      "Epoch 8, Batch 10, Training Loss: 0.018297549337148666\n",
      "Epoch 8, Batch 20, Training Loss: 0.02676287293434143\n",
      "Epoch 8, Batch 30, Training Loss: 0.0331287756562233\n",
      "Epoch 8, Batch 40, Training Loss: 0.026143647730350494\n",
      "Epoch 8, Batch 50, Training Loss: 0.048659950494766235\n",
      "Epoch 8, Batch 60, Training Loss: 0.015186812728643417\n",
      "Epoch 8, Batch 70, Training Loss: 0.04661034792661667\n",
      "Epoch 8, Batch 80, Training Loss: 0.024201244115829468\n",
      "Epoch 8, Batch 90, Training Loss: 0.013819723390042782\n",
      "Epoch 8, Batch 100, Training Loss: 0.05663538724184036\n",
      "Epoch 8, Batch 110, Training Loss: 0.021636618301272392\n",
      "Epoch 8, Batch 120, Training Loss: 0.024946145713329315\n",
      "Epoch 8, Batch 130, Training Loss: 0.019771456718444824\n",
      "Epoch 8, Batch 140, Training Loss: 0.0175706148147583\n",
      "Epoch 8, Batch 150, Training Loss: 0.0305582694709301\n",
      "Epoch 8, Batch 160, Training Loss: 0.023353181779384613\n",
      "Epoch 8, Batch 170, Training Loss: 0.01486431248486042\n",
      "Epoch 8, Batch 180, Training Loss: 0.01961452327668667\n",
      "Epoch 8, Batch 190, Training Loss: 0.018326692283153534\n",
      "Epoch 8, Batch 200, Training Loss: 0.03950775787234306\n",
      "Epoch 8, Batch 210, Training Loss: 0.021453849971294403\n",
      "Epoch 8, Batch 220, Training Loss: 0.023055769503116608\n",
      "Epoch 8, Batch 230, Training Loss: 0.027428150177001953\n",
      "Epoch 8, Batch 240, Training Loss: 0.02757323533296585\n",
      "Epoch 8, Batch 250, Training Loss: 0.01818344183266163\n",
      "Epoch 8, Batch 260, Training Loss: 0.023122726008296013\n",
      "Epoch 8, Batch 270, Training Loss: 0.022893026471138\n",
      "Epoch 8, Batch 280, Training Loss: 0.022486507892608643\n",
      "Epoch 8, Batch 290, Training Loss: 0.014745168387889862\n",
      "Epoch 8, Batch 300, Training Loss: 0.013416142202913761\n",
      "Epoch 8, Batch 310, Training Loss: 0.01842368021607399\n",
      "Epoch 8, Batch 320, Training Loss: 0.028115451335906982\n",
      "Epoch 8, Batch 330, Training Loss: 0.014930803328752518\n",
      "Epoch 8, Batch 340, Training Loss: 0.05124359950423241\n",
      "Epoch 8, Batch 350, Training Loss: 0.01516963541507721\n",
      "Epoch 8, Batch 360, Training Loss: 0.07171191275119781\n",
      "Epoch 8, Batch 370, Training Loss: 0.02001192606985569\n",
      "Epoch 8, Batch 380, Training Loss: 0.01842428557574749\n",
      "Epoch 8, Batch 390, Training Loss: 0.020850827917456627\n",
      "Epoch 8, Batch 400, Training Loss: 0.024592194706201553\n",
      "Epoch 8, Batch 410, Training Loss: 0.03343048319220543\n",
      "Epoch 8, Batch 420, Training Loss: 0.017224876210093498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Batch 430, Training Loss: 0.08608651906251907\n",
      "Epoch 8, Batch 440, Training Loss: 0.01300623919814825\n",
      "Epoch 8, Batch 450, Training Loss: 0.029971685260534286\n",
      "Epoch 8, Batch 460, Training Loss: 0.013588616624474525\n",
      "Epoch 8, Batch 470, Training Loss: 0.022155288606882095\n",
      "Epoch 8, Batch 480, Training Loss: 0.10222416371107101\n",
      "Epoch 8, Batch 490, Training Loss: 0.01948484033346176\n",
      "Epoch 8, Batch 500, Training Loss: 0.021339159458875656\n",
      "Epoch 8, Average Recall: 0.4328210958989748\n",
      "Epoch 9, Batch 10, Training Loss: 0.009664938785135746\n",
      "Epoch 9, Batch 20, Training Loss: 0.0141132278367877\n",
      "Epoch 9, Batch 30, Training Loss: 0.01643122360110283\n",
      "Epoch 9, Batch 40, Training Loss: 0.03791600465774536\n",
      "Epoch 9, Batch 50, Training Loss: 0.011583283543586731\n",
      "Epoch 9, Batch 60, Training Loss: 0.011461431160569191\n",
      "Epoch 9, Batch 70, Training Loss: 0.0613429918885231\n",
      "Epoch 9, Batch 80, Training Loss: 0.016065236181020737\n",
      "Epoch 9, Batch 90, Training Loss: 0.007861841470003128\n",
      "Epoch 9, Batch 100, Training Loss: 0.026531726121902466\n",
      "Epoch 9, Batch 110, Training Loss: 0.01822230964899063\n",
      "Epoch 9, Batch 120, Training Loss: 0.01672794669866562\n",
      "Epoch 9, Batch 130, Training Loss: 0.018761221319437027\n",
      "Epoch 9, Batch 140, Training Loss: 0.010536863468587399\n",
      "Epoch 9, Batch 150, Training Loss: 0.008088224567472935\n",
      "Epoch 9, Batch 160, Training Loss: 0.009239437058568\n",
      "Epoch 9, Batch 170, Training Loss: 0.015317017212510109\n",
      "Epoch 9, Batch 180, Training Loss: 0.015580760315060616\n",
      "Epoch 9, Batch 190, Training Loss: 0.010756955482065678\n",
      "Epoch 9, Batch 200, Training Loss: 0.020076343789696693\n",
      "Epoch 9, Batch 210, Training Loss: 0.02152712270617485\n",
      "Epoch 9, Batch 220, Training Loss: 0.016087690368294716\n",
      "Epoch 9, Batch 230, Training Loss: 0.04377284273505211\n",
      "Epoch 9, Batch 240, Training Loss: 0.009045267477631569\n",
      "Epoch 9, Batch 250, Training Loss: 0.020902534946799278\n",
      "Epoch 9, Batch 260, Training Loss: 0.007925126701593399\n",
      "Epoch 9, Batch 270, Training Loss: 0.017611848190426826\n",
      "Epoch 9, Batch 280, Training Loss: 0.01692294329404831\n",
      "Epoch 9, Batch 290, Training Loss: 0.01130603812634945\n",
      "Epoch 9, Batch 300, Training Loss: 0.014206881634891033\n",
      "Epoch 9, Batch 310, Training Loss: 0.01694207265973091\n",
      "Epoch 9, Batch 320, Training Loss: 0.056567490100860596\n",
      "Epoch 9, Batch 330, Training Loss: 0.009151767008006573\n",
      "Epoch 9, Batch 340, Training Loss: 0.018190531060099602\n",
      "Epoch 9, Batch 350, Training Loss: 0.01290444191545248\n",
      "Epoch 9, Batch 360, Training Loss: 0.013609539717435837\n",
      "Epoch 9, Batch 370, Training Loss: 0.010606242343783379\n",
      "Epoch 9, Batch 380, Training Loss: 0.017628921195864677\n",
      "Epoch 9, Batch 390, Training Loss: 0.017100634053349495\n",
      "Epoch 9, Batch 400, Training Loss: 0.01715931110084057\n",
      "Epoch 9, Batch 410, Training Loss: 0.010146982967853546\n",
      "Epoch 9, Batch 420, Training Loss: 0.026902951300144196\n",
      "Epoch 9, Batch 430, Training Loss: 0.01731131784617901\n",
      "Epoch 9, Batch 440, Training Loss: 0.014567140489816666\n",
      "Epoch 9, Batch 450, Training Loss: 0.011837314814329147\n",
      "Epoch 9, Batch 460, Training Loss: 0.010835086926817894\n",
      "Epoch 9, Batch 470, Training Loss: 0.03300875425338745\n",
      "Epoch 9, Batch 480, Training Loss: 0.0363793671131134\n",
      "Epoch 9, Batch 490, Training Loss: 0.0174916572868824\n",
      "Epoch 9, Batch 500, Training Loss: 0.08145240694284439\n",
      "Epoch 9, Average Recall: 0.43227017691922975\n",
      "Epoch 10, Batch 10, Training Loss: 0.008385087363421917\n",
      "Epoch 10, Batch 20, Training Loss: 0.01171465776860714\n",
      "Epoch 10, Batch 30, Training Loss: 0.0074182841926813126\n",
      "Epoch 10, Batch 40, Training Loss: 0.005708679556846619\n",
      "Epoch 10, Batch 50, Training Loss: 0.005619332194328308\n",
      "Epoch 10, Batch 60, Training Loss: 0.01531907171010971\n",
      "Epoch 10, Batch 70, Training Loss: 0.009965822100639343\n",
      "Epoch 10, Batch 80, Training Loss: 0.041003670543432236\n",
      "Epoch 10, Batch 90, Training Loss: 0.002147509017959237\n",
      "Epoch 10, Batch 100, Training Loss: 0.04027387499809265\n",
      "Epoch 10, Batch 110, Training Loss: 0.0112466374412179\n",
      "Epoch 10, Batch 120, Training Loss: 0.011326022446155548\n",
      "Epoch 10, Batch 130, Training Loss: 0.006703486666083336\n",
      "Epoch 10, Batch 140, Training Loss: 0.014101361855864525\n",
      "Epoch 10, Batch 150, Training Loss: 0.00572270667180419\n",
      "Epoch 10, Batch 160, Training Loss: 0.015691908076405525\n",
      "Epoch 10, Batch 170, Training Loss: 0.01277164276689291\n",
      "Epoch 10, Batch 180, Training Loss: 0.00792457815259695\n",
      "Epoch 10, Batch 190, Training Loss: 0.012511609122157097\n",
      "Epoch 10, Batch 200, Training Loss: 0.012874152511358261\n",
      "Epoch 10, Batch 210, Training Loss: 0.025605427101254463\n",
      "Epoch 10, Batch 220, Training Loss: 0.0062472764402627945\n",
      "Epoch 10, Batch 230, Training Loss: 0.01322986651211977\n",
      "Epoch 10, Batch 240, Training Loss: 0.011818451806902885\n",
      "Epoch 10, Batch 250, Training Loss: 0.02264510467648506\n",
      "Epoch 10, Batch 260, Training Loss: 0.012089869007468224\n",
      "Epoch 10, Batch 270, Training Loss: 0.01177280768752098\n",
      "Epoch 10, Batch 280, Training Loss: 0.031370848417282104\n",
      "Epoch 10, Batch 290, Training Loss: 0.00973489135503769\n",
      "Epoch 10, Batch 300, Training Loss: 0.008719862438738346\n",
      "Epoch 10, Batch 310, Training Loss: 0.015301893465220928\n",
      "Epoch 10, Batch 320, Training Loss: 0.0649368092417717\n",
      "Epoch 10, Batch 330, Training Loss: 0.00864027813076973\n",
      "Epoch 10, Batch 340, Training Loss: 0.06809770315885544\n",
      "Epoch 10, Batch 350, Training Loss: 0.010540822520852089\n",
      "Epoch 10, Batch 360, Training Loss: 0.0053451452404260635\n",
      "Epoch 10, Batch 370, Training Loss: 0.0058015030808746815\n",
      "Epoch 10, Batch 380, Training Loss: 0.003947342745959759\n",
      "Epoch 10, Batch 390, Training Loss: 0.013123649172484875\n",
      "Epoch 10, Batch 400, Training Loss: 0.03316499665379524\n",
      "Epoch 10, Batch 410, Training Loss: 0.00869110319763422\n",
      "Epoch 10, Batch 420, Training Loss: 0.02118617668747902\n",
      "Epoch 10, Batch 430, Training Loss: 0.011626320891082287\n",
      "Epoch 10, Batch 440, Training Loss: 0.00768396956846118\n",
      "Epoch 10, Batch 450, Training Loss: 0.00695467134937644\n",
      "Epoch 10, Batch 460, Training Loss: 0.010637147352099419\n",
      "Epoch 10, Batch 470, Training Loss: 0.0062249028123915195\n",
      "Epoch 10, Batch 480, Training Loss: 0.008248871192336082\n",
      "Epoch 10, Batch 490, Training Loss: 0.00600148132070899\n",
      "Epoch 10, Batch 500, Training Loss: 0.009738135151565075\n",
      "Epoch 10, Average Recall: 0.4317876344086022\n",
      "Epoch 1, Batch 10, Training Loss: 0.7025161981582642\n",
      "Epoch 1, Batch 20, Training Loss: 0.7769510746002197\n",
      "Epoch 1, Batch 30, Training Loss: 0.7605980038642883\n",
      "Epoch 1, Average Recall: 0.33549679487179485\n",
      "Epoch 2, Batch 10, Training Loss: 0.5130456686019897\n",
      "Epoch 2, Batch 20, Training Loss: 0.3981127142906189\n",
      "Epoch 2, Batch 30, Training Loss: 0.5215990543365479\n",
      "Epoch 2, Average Recall: 0.4169337606837607\n",
      "Epoch 3, Batch 10, Training Loss: 0.42991071939468384\n",
      "Epoch 3, Batch 20, Training Loss: 0.4599800109863281\n",
      "Epoch 3, Batch 30, Training Loss: 0.44513240456581116\n",
      "Epoch 3, Average Recall: 0.4533644943019943\n",
      "Epoch 4, Batch 10, Training Loss: 0.3559072017669678\n",
      "Epoch 4, Batch 20, Training Loss: 0.26351574063301086\n",
      "Epoch 4, Batch 30, Training Loss: 0.26995599269866943\n",
      "Epoch 4, Average Recall: 0.4699617165242166\n",
      "Epoch 5, Batch 10, Training Loss: 0.2624413073062897\n",
      "Epoch 5, Batch 20, Training Loss: 0.17716801166534424\n",
      "Epoch 5, Batch 30, Training Loss: 0.20432215929031372\n",
      "Epoch 5, Average Recall: 0.4780172720797721\n",
      "Epoch 6, Batch 10, Training Loss: 0.22778929769992828\n",
      "Epoch 6, Batch 20, Training Loss: 0.20407453179359436\n",
      "Epoch 6, Batch 30, Training Loss: 0.1361090987920761\n",
      "Epoch 6, Average Recall: 0.48284366096866094\n",
      "Epoch 7, Batch 10, Training Loss: 0.22726242244243622\n",
      "Epoch 7, Batch 20, Training Loss: 0.15871140360832214\n",
      "Epoch 7, Batch 30, Training Loss: 0.18442215025424957\n",
      "Epoch 7, Average Recall: 0.48971955128205136\n",
      "Epoch 8, Batch 10, Training Loss: 0.16988693177700043\n",
      "Epoch 8, Batch 20, Training Loss: 0.1680871546268463\n",
      "Epoch 8, Batch 30, Training Loss: 0.11251889169216156\n",
      "Epoch 8, Average Recall: 0.4912126068376068\n",
      "Epoch 9, Batch 10, Training Loss: 0.12879008054733276\n",
      "Epoch 9, Batch 20, Training Loss: 0.10749612748622894\n",
      "Epoch 9, Batch 30, Training Loss: 0.14322270452976227\n",
      "Epoch 9, Average Recall: 0.4946144943019942\n",
      "Epoch 10, Batch 10, Training Loss: 0.07766447961330414\n",
      "Epoch 10, Batch 20, Training Loss: 0.08238756656646729\n",
      "Epoch 10, Batch 30, Training Loss: 0.06693227589130402\n",
      "Epoch 10, Average Recall: 0.4928783831908831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 10, Training Loss: 0.744601309299469\n",
      "Epoch 1, Batch 20, Training Loss: 0.7018846869468689\n",
      "Epoch 1, Batch 30, Training Loss: 0.7240990400314331\n",
      "Epoch 1, Batch 40, Training Loss: 0.6189072728157043\n",
      "Epoch 1, Batch 50, Training Loss: 0.6541412472724915\n",
      "Epoch 1, Batch 60, Training Loss: 0.6191960573196411\n",
      "Epoch 1, Batch 70, Training Loss: 0.61759352684021\n",
      "Epoch 1, Batch 80, Training Loss: 0.6121242046356201\n",
      "Epoch 1, Batch 90, Training Loss: 0.5798351168632507\n",
      "Epoch 1, Batch 100, Training Loss: 0.6163702011108398\n",
      "Epoch 1, Batch 110, Training Loss: 0.5497775673866272\n",
      "Epoch 1, Batch 120, Training Loss: 0.6139779686927795\n",
      "Epoch 1, Batch 130, Training Loss: 0.49913084506988525\n",
      "Epoch 1, Batch 140, Training Loss: 0.5004692077636719\n",
      "Epoch 1, Batch 150, Training Loss: 0.5781500935554504\n",
      "Epoch 1, Batch 160, Training Loss: 0.5666325688362122\n",
      "Epoch 1, Batch 170, Training Loss: 0.4382256865501404\n",
      "Epoch 1, Batch 180, Training Loss: 0.5446555614471436\n",
      "Epoch 1, Batch 190, Training Loss: 0.6494817733764648\n",
      "Epoch 1, Batch 200, Training Loss: 0.48679420351982117\n",
      "Epoch 1, Batch 210, Training Loss: 0.49409714341163635\n",
      "Epoch 1, Batch 220, Training Loss: 0.4657667279243469\n",
      "Epoch 1, Batch 230, Training Loss: 0.42748749256134033\n",
      "Epoch 1, Batch 240, Training Loss: 0.4976992607116699\n",
      "Epoch 1, Batch 250, Training Loss: 0.5490435361862183\n",
      "Epoch 1, Average Recall: 0.3890148326701357\n",
      "Epoch 2, Batch 10, Training Loss: 0.4588364362716675\n",
      "Epoch 2, Batch 20, Training Loss: 0.43757784366607666\n",
      "Epoch 2, Batch 30, Training Loss: 0.4472465217113495\n",
      "Epoch 2, Batch 40, Training Loss: 0.3541475236415863\n",
      "Epoch 2, Batch 50, Training Loss: 0.47837772965431213\n",
      "Epoch 2, Batch 60, Training Loss: 0.4743514358997345\n",
      "Epoch 2, Batch 70, Training Loss: 0.43014246225357056\n",
      "Epoch 2, Batch 80, Training Loss: 0.4428653120994568\n",
      "Epoch 2, Batch 90, Training Loss: 0.41506078839302063\n",
      "Epoch 2, Batch 100, Training Loss: 0.5156053900718689\n",
      "Epoch 2, Batch 110, Training Loss: 0.5067986845970154\n",
      "Epoch 2, Batch 120, Training Loss: 0.3201625347137451\n",
      "Epoch 2, Batch 130, Training Loss: 0.376619428396225\n",
      "Epoch 2, Batch 140, Training Loss: 0.3345452845096588\n",
      "Epoch 2, Batch 150, Training Loss: 0.4283682107925415\n",
      "Epoch 2, Batch 160, Training Loss: 0.44267162680625916\n",
      "Epoch 2, Batch 170, Training Loss: 0.4503495395183563\n",
      "Epoch 2, Batch 180, Training Loss: 0.34139931201934814\n",
      "Epoch 2, Batch 190, Training Loss: 0.5004316568374634\n",
      "Epoch 2, Batch 200, Training Loss: 0.3739776909351349\n",
      "Epoch 2, Batch 210, Training Loss: 0.46384143829345703\n",
      "Epoch 2, Batch 220, Training Loss: 0.4079606533050537\n",
      "Epoch 2, Batch 230, Training Loss: 0.35279470682144165\n",
      "Epoch 2, Batch 240, Training Loss: 0.364888459444046\n",
      "Epoch 2, Batch 250, Training Loss: 0.29899075627326965\n",
      "Epoch 2, Average Recall: 0.40424326918171616\n",
      "Epoch 3, Batch 10, Training Loss: 0.38801994919776917\n",
      "Epoch 3, Batch 20, Training Loss: 0.37016916275024414\n",
      "Epoch 3, Batch 30, Training Loss: 0.3310180604457855\n",
      "Epoch 3, Batch 40, Training Loss: 0.3130672574043274\n",
      "Epoch 3, Batch 50, Training Loss: 0.42159977555274963\n",
      "Epoch 3, Batch 60, Training Loss: 0.39449363946914673\n",
      "Epoch 3, Batch 70, Training Loss: 0.32889580726623535\n",
      "Epoch 3, Batch 80, Training Loss: 0.4221179485321045\n",
      "Epoch 3, Batch 90, Training Loss: 0.28606605529785156\n",
      "Epoch 3, Batch 100, Training Loss: 0.415654718875885\n",
      "Epoch 3, Batch 110, Training Loss: 0.34632059931755066\n",
      "Epoch 3, Batch 120, Training Loss: 0.24836061894893646\n",
      "Epoch 3, Batch 130, Training Loss: 0.3755545914173126\n",
      "Epoch 3, Batch 140, Training Loss: 0.2883652448654175\n",
      "Epoch 3, Batch 150, Training Loss: 0.38124287128448486\n",
      "Epoch 3, Batch 160, Training Loss: 0.2686202824115753\n",
      "Epoch 3, Batch 170, Training Loss: 0.2934856712818146\n",
      "Epoch 3, Batch 180, Training Loss: 0.23319727182388306\n",
      "Epoch 3, Batch 190, Training Loss: 0.30755993723869324\n",
      "Epoch 3, Batch 200, Training Loss: 0.3053130805492401\n",
      "Epoch 3, Batch 210, Training Loss: 0.408909410238266\n",
      "Epoch 3, Batch 220, Training Loss: 0.3330864906311035\n",
      "Epoch 3, Batch 230, Training Loss: 0.3717474937438965\n",
      "Epoch 3, Batch 240, Training Loss: 0.3811279535293579\n",
      "Epoch 3, Batch 250, Training Loss: 0.36569133400917053\n",
      "Epoch 3, Average Recall: 0.4097102655341292\n",
      "Epoch 4, Batch 10, Training Loss: 0.2701505720615387\n",
      "Epoch 4, Batch 20, Training Loss: 0.22784638404846191\n",
      "Epoch 4, Batch 30, Training Loss: 0.3195735216140747\n",
      "Epoch 4, Batch 40, Training Loss: 0.4660303294658661\n",
      "Epoch 4, Batch 50, Training Loss: 0.21881833672523499\n",
      "Epoch 4, Batch 60, Training Loss: 0.2373572289943695\n",
      "Epoch 4, Batch 70, Training Loss: 0.1754215955734253\n",
      "Epoch 4, Batch 80, Training Loss: 0.2839447259902954\n",
      "Epoch 4, Batch 90, Training Loss: 0.22051163017749786\n",
      "Epoch 4, Batch 100, Training Loss: 0.23929986357688904\n",
      "Epoch 4, Batch 110, Training Loss: 0.14890646934509277\n",
      "Epoch 4, Batch 120, Training Loss: 0.1857738494873047\n",
      "Epoch 4, Batch 130, Training Loss: 0.3359505832195282\n",
      "Epoch 4, Batch 140, Training Loss: 0.37558513879776\n",
      "Epoch 4, Batch 150, Training Loss: 0.26736146211624146\n",
      "Epoch 4, Batch 160, Training Loss: 0.35949093103408813\n",
      "Epoch 4, Batch 170, Training Loss: 0.20259076356887817\n",
      "Epoch 4, Batch 180, Training Loss: 0.2554953694343567\n",
      "Epoch 4, Batch 190, Training Loss: 0.25362062454223633\n",
      "Epoch 4, Batch 200, Training Loss: 0.31694495677948\n",
      "Epoch 4, Batch 210, Training Loss: 0.17398396134376526\n",
      "Epoch 4, Batch 220, Training Loss: 0.33720913529396057\n",
      "Epoch 4, Batch 230, Training Loss: 0.33551904559135437\n",
      "Epoch 4, Batch 240, Training Loss: 0.20775941014289856\n",
      "Epoch 4, Batch 250, Training Loss: 0.19827985763549805\n",
      "Epoch 4, Average Recall: 0.41295012945107645\n",
      "Epoch 5, Batch 10, Training Loss: 0.3182244300842285\n",
      "Epoch 5, Batch 20, Training Loss: 0.18918585777282715\n",
      "Epoch 5, Batch 30, Training Loss: 0.18651515245437622\n",
      "Epoch 5, Batch 40, Training Loss: 0.16185078024864197\n",
      "Epoch 5, Batch 50, Training Loss: 0.22417616844177246\n",
      "Epoch 5, Batch 60, Training Loss: 0.1671348512172699\n",
      "Epoch 5, Batch 70, Training Loss: 0.13303789496421814\n",
      "Epoch 5, Batch 80, Training Loss: 0.2583808898925781\n",
      "Epoch 5, Batch 90, Training Loss: 0.19609875977039337\n",
      "Epoch 5, Batch 100, Training Loss: 0.19339829683303833\n",
      "Epoch 5, Batch 110, Training Loss: 0.22697237133979797\n",
      "Epoch 5, Batch 120, Training Loss: 0.2230328917503357\n",
      "Epoch 5, Batch 130, Training Loss: 0.18576623499393463\n",
      "Epoch 5, Batch 140, Training Loss: 0.23303942382335663\n",
      "Epoch 5, Batch 150, Training Loss: 0.22871127724647522\n",
      "Epoch 5, Batch 160, Training Loss: 0.2969666123390198\n",
      "Epoch 5, Batch 170, Training Loss: 0.2172933965921402\n",
      "Epoch 5, Batch 180, Training Loss: 0.1604691594839096\n",
      "Epoch 5, Batch 190, Training Loss: 0.19211508333683014\n",
      "Epoch 5, Batch 200, Training Loss: 0.2191154658794403\n",
      "Epoch 5, Batch 210, Training Loss: 0.15404227375984192\n",
      "Epoch 5, Batch 220, Training Loss: 0.26772022247314453\n",
      "Epoch 5, Batch 230, Training Loss: 0.16705310344696045\n",
      "Epoch 5, Batch 240, Training Loss: 0.1625276356935501\n",
      "Epoch 5, Batch 250, Training Loss: 0.23472918570041656\n",
      "Epoch 5, Average Recall: 0.4151772618865422\n",
      "Epoch 6, Batch 10, Training Loss: 0.1322949230670929\n",
      "Epoch 6, Batch 20, Training Loss: 0.15360665321350098\n",
      "Epoch 6, Batch 30, Training Loss: 0.12350527197122574\n",
      "Epoch 6, Batch 40, Training Loss: 0.12408390641212463\n",
      "Epoch 6, Batch 50, Training Loss: 0.1320934295654297\n",
      "Epoch 6, Batch 60, Training Loss: 0.1623796671628952\n",
      "Epoch 6, Batch 70, Training Loss: 0.14066675305366516\n",
      "Epoch 6, Batch 80, Training Loss: 0.12873905897140503\n",
      "Epoch 6, Batch 90, Training Loss: 0.17424270510673523\n",
      "Epoch 6, Batch 100, Training Loss: 0.10085231065750122\n",
      "Epoch 6, Batch 110, Training Loss: 0.14124314486980438\n",
      "Epoch 6, Batch 120, Training Loss: 0.14928963780403137\n",
      "Epoch 6, Batch 130, Training Loss: 0.15556663274765015\n",
      "Epoch 6, Batch 140, Training Loss: 0.1843738853931427\n",
      "Epoch 6, Batch 150, Training Loss: 0.10386990010738373\n",
      "Epoch 6, Batch 160, Training Loss: 0.1358538120985031\n",
      "Epoch 6, Batch 170, Training Loss: 0.15227574110031128\n",
      "Epoch 6, Batch 180, Training Loss: 0.18986748158931732\n",
      "Epoch 6, Batch 190, Training Loss: 0.13765273988246918\n",
      "Epoch 6, Batch 200, Training Loss: 0.19586262106895447\n",
      "Epoch 6, Batch 210, Training Loss: 0.16193990409374237\n",
      "Epoch 6, Batch 220, Training Loss: 0.1401652991771698\n",
      "Epoch 6, Batch 230, Training Loss: 0.12879906594753265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Batch 240, Training Loss: 0.13768932223320007\n",
      "Epoch 6, Batch 250, Training Loss: 0.14043350517749786\n",
      "Epoch 6, Average Recall: 0.4152968287674727\n",
      "Epoch 7, Batch 10, Training Loss: 0.09865707904100418\n",
      "Epoch 7, Batch 20, Training Loss: 0.06904307007789612\n",
      "Epoch 7, Batch 30, Training Loss: 0.095075324177742\n",
      "Epoch 7, Batch 40, Training Loss: 0.1469830870628357\n",
      "Epoch 7, Batch 50, Training Loss: 0.11210278421640396\n",
      "Epoch 7, Batch 60, Training Loss: 0.07636594772338867\n",
      "Epoch 7, Batch 70, Training Loss: 0.09504079073667526\n",
      "Epoch 7, Batch 80, Training Loss: 0.10471309721469879\n",
      "Epoch 7, Batch 90, Training Loss: 0.06993618607521057\n",
      "Epoch 7, Batch 100, Training Loss: 0.1793447881937027\n",
      "Epoch 7, Batch 110, Training Loss: 0.10395123064517975\n",
      "Epoch 7, Batch 120, Training Loss: 0.09533260017633438\n",
      "Epoch 7, Batch 130, Training Loss: 0.07118261605501175\n",
      "Epoch 7, Batch 140, Training Loss: 0.08060947060585022\n",
      "Epoch 7, Batch 150, Training Loss: 0.14614491164684296\n",
      "Epoch 7, Batch 160, Training Loss: 0.13715876638889313\n",
      "Epoch 7, Batch 170, Training Loss: 0.12977321445941925\n",
      "Epoch 7, Batch 180, Training Loss: 0.1000542864203453\n",
      "Epoch 7, Batch 190, Training Loss: 0.07373219728469849\n",
      "Epoch 7, Batch 200, Training Loss: 0.10166241228580475\n",
      "Epoch 7, Batch 210, Training Loss: 0.103996641933918\n",
      "Epoch 7, Batch 220, Training Loss: 0.11657664179801941\n",
      "Epoch 7, Batch 230, Training Loss: 0.09812860190868378\n",
      "Epoch 7, Batch 240, Training Loss: 0.09774070233106613\n",
      "Epoch 7, Batch 250, Training Loss: 0.07145626842975616\n",
      "Epoch 7, Average Recall: 0.41695402573716966\n",
      "Epoch 8, Batch 10, Training Loss: 0.0722694844007492\n",
      "Epoch 8, Batch 20, Training Loss: 0.04092758521437645\n",
      "Epoch 8, Batch 30, Training Loss: 0.06735402345657349\n",
      "Epoch 8, Batch 40, Training Loss: 0.09026312083005905\n",
      "Epoch 8, Batch 50, Training Loss: 0.06485091149806976\n",
      "Epoch 8, Batch 60, Training Loss: 0.08537720888853073\n",
      "Epoch 8, Batch 70, Training Loss: 0.07897837460041046\n",
      "Epoch 8, Batch 80, Training Loss: 0.07457637041807175\n",
      "Epoch 8, Batch 90, Training Loss: 0.13519124686717987\n",
      "Epoch 8, Batch 100, Training Loss: 0.08998820930719376\n",
      "Epoch 8, Batch 110, Training Loss: 0.056671854108572006\n",
      "Epoch 8, Batch 120, Training Loss: 0.07424227893352509\n",
      "Epoch 8, Batch 130, Training Loss: 0.07222451269626617\n",
      "Epoch 8, Batch 140, Training Loss: 0.0806678980588913\n",
      "Epoch 8, Batch 150, Training Loss: 0.07553183287382126\n",
      "Epoch 8, Batch 160, Training Loss: 0.044106822460889816\n",
      "Epoch 8, Batch 170, Training Loss: 0.05524582043290138\n",
      "Epoch 8, Batch 180, Training Loss: 0.04995417222380638\n",
      "Epoch 8, Batch 190, Training Loss: 0.059835389256477356\n",
      "Epoch 8, Batch 200, Training Loss: 0.05644632875919342\n",
      "Epoch 8, Batch 210, Training Loss: 0.0745367556810379\n",
      "Epoch 8, Batch 220, Training Loss: 0.060503341257572174\n",
      "Epoch 8, Batch 230, Training Loss: 0.057986609637737274\n",
      "Epoch 8, Batch 240, Training Loss: 0.05758914351463318\n",
      "Epoch 8, Batch 250, Training Loss: 0.06296990811824799\n",
      "Epoch 8, Average Recall: 0.41666268110396903\n",
      "Epoch 9, Batch 10, Training Loss: 0.05859578400850296\n",
      "Epoch 9, Batch 20, Training Loss: 0.03545964136719704\n",
      "Epoch 9, Batch 30, Training Loss: 0.03493933379650116\n",
      "Epoch 9, Batch 40, Training Loss: 0.054273270070552826\n",
      "Epoch 9, Batch 50, Training Loss: 0.03977569192647934\n",
      "Epoch 9, Batch 60, Training Loss: 0.04440900310873985\n",
      "Epoch 9, Batch 70, Training Loss: 0.03954101353883743\n",
      "Epoch 9, Batch 80, Training Loss: 0.03955051302909851\n",
      "Epoch 9, Batch 90, Training Loss: 0.034151557832956314\n",
      "Epoch 9, Batch 100, Training Loss: 0.030698785558342934\n",
      "Epoch 9, Batch 110, Training Loss: 0.07331010699272156\n",
      "Epoch 9, Batch 120, Training Loss: 0.0556342788040638\n",
      "Epoch 9, Batch 130, Training Loss: 0.04919666796922684\n",
      "Epoch 9, Batch 140, Training Loss: 0.029217537492513657\n",
      "Epoch 9, Batch 150, Training Loss: 0.023033112287521362\n",
      "Epoch 9, Batch 160, Training Loss: 0.06069160997867584\n",
      "Epoch 9, Batch 170, Training Loss: 0.07990400493144989\n",
      "Epoch 9, Batch 180, Training Loss: 0.03453771397471428\n",
      "Epoch 9, Batch 190, Training Loss: 0.04244397580623627\n",
      "Epoch 9, Batch 200, Training Loss: 0.046286486089229584\n",
      "Epoch 9, Batch 210, Training Loss: 0.024030763655900955\n",
      "Epoch 9, Batch 220, Training Loss: 0.03135618567466736\n",
      "Epoch 9, Batch 230, Training Loss: 0.026670532301068306\n",
      "Epoch 9, Batch 240, Training Loss: 0.04321319982409477\n",
      "Epoch 9, Batch 250, Training Loss: 0.03354629874229431\n",
      "Epoch 9, Average Recall: 0.41615731175390264\n",
      "Epoch 10, Batch 10, Training Loss: 0.036579713225364685\n",
      "Epoch 10, Batch 20, Training Loss: 0.024885907769203186\n",
      "Epoch 10, Batch 30, Training Loss: 0.036723509430885315\n",
      "Epoch 10, Batch 40, Training Loss: 0.0294097438454628\n",
      "Epoch 10, Batch 50, Training Loss: 0.026729010045528412\n",
      "Epoch 10, Batch 60, Training Loss: 0.02344021573662758\n",
      "Epoch 10, Batch 70, Training Loss: 0.04507714509963989\n",
      "Epoch 10, Batch 80, Training Loss: 0.02506123296916485\n",
      "Epoch 10, Batch 90, Training Loss: 0.01993304304778576\n",
      "Epoch 10, Batch 100, Training Loss: 0.0216201301664114\n",
      "Epoch 10, Batch 110, Training Loss: 0.015970146283507347\n",
      "Epoch 10, Batch 120, Training Loss: 0.022275937721133232\n",
      "Epoch 10, Batch 130, Training Loss: 0.011684481054544449\n",
      "Epoch 10, Batch 140, Training Loss: 0.0193751510232687\n",
      "Epoch 10, Batch 150, Training Loss: 0.026793058961629868\n",
      "Epoch 10, Batch 160, Training Loss: 0.025704400613904\n",
      "Epoch 10, Batch 170, Training Loss: 0.03158769756555557\n",
      "Epoch 10, Batch 180, Training Loss: 0.024373501539230347\n",
      "Epoch 10, Batch 190, Training Loss: 0.02188533917069435\n",
      "Epoch 10, Batch 200, Training Loss: 0.058270156383514404\n",
      "Epoch 10, Batch 210, Training Loss: 0.05597205087542534\n",
      "Epoch 10, Batch 220, Training Loss: 0.026738706976175308\n",
      "Epoch 10, Batch 230, Training Loss: 0.034313689917325974\n",
      "Epoch 10, Batch 240, Training Loss: 0.029337909072637558\n",
      "Epoch 10, Batch 250, Training Loss: 0.024528225883841515\n",
      "Epoch 10, Average Recall: 0.4156411813845526\n"
     ]
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "train_df = pd.read_csv('train_data_set_food.csv')\n",
    "test_df = pd.read_csv('test_data_set_food.csv')\n",
    "\n",
    "# 재료 목록 정의\n",
    "ingredients = ['chicken', 'shrimp', 'cheese', 'pork', 'cream', 'tofu', 'lobster', 'peanut', 'bread', 'crab', 'egg']\n",
    "\n",
    "for ingredient in ingredients:\n",
    "    # 데이터 선택 및 라벨링\n",
    "    positive_train_df = train_df[train_df[ingredient] == 1]\n",
    "    negative_train_df = train_df[train_df[ingredient] == 0].sample(n=len(positive_train_df), random_state=42)\n",
    "    positive_test_df = test_df[test_df[ingredient] == 1]\n",
    "    negative_test_df = test_df[test_df[ingredient] == 0].sample(n=len(positive_test_df), random_state=42)\n",
    "\n",
    "    balanced_train_df = pd.concat([positive_train_df, negative_train_df])\n",
    "    balanced_test_df = pd.concat([positive_test_df, negative_test_df])\n",
    "\n",
    "    balanced_train_df['label'] = balanced_train_df[ingredient]\n",
    "    balanced_test_df['label'] = balanced_test_df[ingredient]\n",
    "\n",
    "    # 이미지 경로 설정\n",
    "    balanced_train_df['image_path'] = balanced_train_df.apply(lambda row: os.path.join('food_dataset/images', row['food_class'], 'png', row['image_name']), axis=1)\n",
    "    balanced_test_df['image_path'] = balanced_test_df.apply(lambda row: os.path.join('food_dataset/images', row['food_class'], 'png', row['image_name']), axis=1)\n",
    "\n",
    "    # 전처리 정의\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # 데이터셋 객체 생성 및 DataLoader 정의\n",
    "    train_dataset = FoodDataset(balanced_train_df, transform=transform)\n",
    "    test_dataset = FoodDataset(balanced_test_df, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    # 모델 초기화\n",
    "    # 모델 초기화\n",
    "    weights = ResNet18_Weights.IMAGENET1K_V1 if torch.cuda.is_available() else ResNet18_Weights.DEFAULT\n",
    "    model = models.resnet18(weights=weights)\n",
    "    model.fc = nn.Linear(model.fc.in_features, 1)  # 이진 분류를 위한 출력 뉴런 1개\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "    \n",
    "    # 학습 파라미터\n",
    "    epochs = 10\n",
    "    \n",
    "    # 모델 학습 및 평가\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            labels = labels.unsqueeze(1).float()  # 레이블 차원 변경\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f'Epoch {epoch+1}, Batch {i+1}, Training Loss: {loss.item()}')\n",
    "\n",
    "        # 모델 평가\n",
    "        model.eval()\n",
    "        total_recall = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                recall = calculate_recall(outputs, labels)\n",
    "                total_recall += recall\n",
    "\n",
    "        average_recall = total_recall / len(test_loader)\n",
    "        print(f'Epoch {epoch+1}, Average Recall: {average_recall}')\n",
    "\n",
    "        # 모델 저장\n",
    "    torch.save(model.state_dict(), f'new_{ingredient}_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d811112d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
